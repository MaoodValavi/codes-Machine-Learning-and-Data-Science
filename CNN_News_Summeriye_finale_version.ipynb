{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-OETqV4Jf3P",
        "outputId": "af9b0f6a-2e6b-4f43-bd3a-db9b4f385581"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#  نصب کتابخانه rouge-score برای محاسبه معیارهای ROUGE\n",
        "!pip install rouge-score --quiet\n",
        "\n",
        "# اتصال گوگل درایو به محیط Google Colab برای دسترسی به فایل‌ها\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVfLPU91JqeK"
      },
      "outputs": [],
      "source": [
        "\n",
        "# کتابخانه‌های مورد نیاز\n",
        "import os                              # برای مدیریت مسیرها و فایل‌ها\n",
        "import tensorflow as tf                # برای ساخت و آموزش مدل‌های یادگیری عمیق\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense, Layer, TextVectorization  # لایه‌های مورد استفاده در مدل\n",
        "import pandas as pd                    # برای کار با داده‌های جدولی (DataFrame)\n",
        "import numpy as np                     # عملیات عددی و آرایه‌ای\n",
        "import re                              # برای کار با عبارات با قاعده (Regular Expressions)\n",
        "from rouge_score import rouge_scorer   # برای محاسبه معیارهای ارزیابی ROUGE\n",
        "\n",
        "# ابرپارامترهای مدل\n",
        "BATCH_SIZE = 64            # تعداد نمونه‌ها در هر دسته آموزشی\n",
        "EMBEDDING_DIM = 256        # اندازه بردارهای تعبیه (Embedding vectors)\n",
        "UNITS = 128                # تعداد واحدهای GRU\n",
        "MAX_VOCAB_SIZE = 5000      # حداکثر تعداد کلمات در واژگان\n",
        "MAX_ARTICLE_LEN = 200      # حداکثر طول ورودی (مقاله)\n",
        "MAX_SUMMARY_LEN = 50       # حداکثر طول خروجی (خلاصه)\n",
        "EPOCHS = 10                # تعداد دوره‌های آموزش مدل\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "og6GdMkKJtM5"
      },
      "outputs": [],
      "source": [
        "#  تابع پاکسازی متن\n",
        "def clean_text(text):\n",
        "    # اگر ورودی بایت باشد → تبدیل به رشته با رمزگشایی UTF-8\n",
        "    if isinstance(text, bytes):\n",
        "        text = text.decode('utf-8', errors='replace')\n",
        "    # اگر ورودی رشته باشد → دوباره رمزگذاری و رمزگشایی برای حذف نویز‌های احتمالی\n",
        "    elif isinstance(text, str):\n",
        "        text = text.encode('utf-8', errors='replace').decode('utf-8', errors='replace')\n",
        "    # حذف کاراکترهای غیر استاندارد (فقط کاراکترهای ASCII و خط جدید باقی می‌مانند)\n",
        "    text = re.sub(r'[^\\x20-\\x7E\\n]', '', text)\n",
        "    return text\n",
        "\n",
        "#  تابع بارگذاری داده‌ها از فایل CSV\n",
        "def load_data(path):\n",
        "    df = pd.read_csv(path)  # خواندن فایل CSV به صورت DataFrame\n",
        "    # استخراج ستون 'article' به عنوان ورودی‌ها و پاکسازی آن‌ها\n",
        "    inputs = [clean_text(t) for t in df['article'].astype(str).tolist()]\n",
        "    # استخراج ستون 'highlights' به عنوان خروجی‌ها و افزودن توکن‌های <start> و <end>\n",
        "    targets = [\"<start> \" + clean_text(t) + \" <end>\" for t in df['highlights'].astype(str).tolist()]\n",
        "    return inputs, targets\n",
        "\n",
        "#  مسیر پوشه دیتاست در Google Drive\n",
        "base_path = '/content/drive/MyDrive/Colab Notebooks/cnn_dailymail'\n",
        "\n",
        "#  بارگذاری داده‌های آموزشی، اعتبارسنجی و آزمون\n",
        "train_inputs, train_targets = load_data(os.path.join(base_path, 'train.csv'))\n",
        "val_inputs, val_targets = load_data(os.path.join(base_path, 'validation.csv'))\n",
        "test_inputs, test_targets = load_data(os.path.join(base_path, 'test.csv'))\n",
        "\n",
        "#  کاهش اندازه دیتاست آموزش برای صرفه‌جویی در زمان آموزش (10٪ داده‌های اصلی)\n",
        "train_inputs = train_inputs[:len(train_inputs) // 10]\n",
        "train_targets = train_targets[:len(train_targets) // 10]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwY7LoJPJwYY"
      },
      "outputs": [],
      "source": [
        "#  ایجاد بردارکننده‌های متنی برای ورودی‌ها (مقالات) و خروجی‌ها (خلاصه‌ها)\n",
        "article_vectorizer = TextVectorization(max_tokens=MAX_VOCAB_SIZE, output_sequence_length=MAX_ARTICLE_LEN)\n",
        "summary_vectorizer = TextVectorization(max_tokens=MAX_VOCAB_SIZE, output_sequence_length=MAX_SUMMARY_LEN)\n",
        "\n",
        "#  آموزش (فیت) بردارکننده‌ها روی داده‌های آموزشی\n",
        "article_vectorizer.adapt(train_inputs)\n",
        "summary_vectorizer.adapt(train_targets)\n",
        "\n",
        "#  استخراج ایندکس توکن‌های <start> و <end> برای استفاده در رمزگشا (decoder)\n",
        "start_token_idx = summary_vectorizer([\"<start>\"]).numpy()[0][0]\n",
        "end_token_idx = summary_vectorizer([\"<end>\"]).numpy()[0][0]\n",
        "\n",
        "#  تابع تبدیل ورودی و خروجی به بردارهای عددی\n",
        "def vectorize(article, summary):\n",
        "    return article_vectorizer(article), summary_vectorizer(summary)\n",
        "\n",
        "#  تابع ساخت Dataset آماده برای آموزش\n",
        "def make_dataset(inputs, targets):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((inputs, targets))  # تبدیل داده‌ها به Dataset تنسورفلو\n",
        "    ds = ds.shuffle(10000)                                     # درهم‌ریزی داده‌ها برای جلوگیری از overfitting\n",
        "    ds = ds.batch(BATCH_SIZE)                                  # تقسیم داده‌ها به batch‌های آموزشی\n",
        "    ds = ds.map(lambda x, y: vectorize(x, y))                  # اعمال بردارسازی به هر نمونه\n",
        "    ds = ds.prefetch(tf.data.AUTOTUNE)                         # بارگذاری پیش‌دستانه برای افزایش سرعت آموزش\n",
        "    return ds\n",
        "\n",
        "# ساخت Dataset‌های آموزشی، اعتبارسنجی و آزمون\n",
        "train_ds = make_dataset(train_inputs, train_targets)\n",
        "val_ds = make_dataset(val_inputs, val_targets)\n",
        "test_ds = make_dataset(test_inputs, test_targets)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ToBChjyJ2oY"
      },
      "outputs": [],
      "source": [
        "#  تعریف مکانیزم توجه به سبک Bahdanau\n",
        "class BahdanauAttention(Layer):\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "        self.W1 = Dense(units)  # لایه متراکم برای query\n",
        "        self.W2 = Dense(units)  # لایه متراکم برای values (خروجی رمزگذار)\n",
        "        self.V = Dense(1)       # لایه متراکم برای محاسبه امتیاز توجه (attention score)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        query = tf.expand_dims(query, 1)  # اضافه کردن بعد برای broadcast\n",
        "        score = tf.nn.tanh(self.W1(query) + self.W2(values))  # محاسبه نمره توجه (attention score)\n",
        "        attention_weights = tf.nn.softmax(self.V(score), axis=1)  # نرمال‌سازی با softmax\n",
        "        context_vector = attention_weights * values  # وزن‌دهی به ویژگی‌ها براساس وزن توجه\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)  # بردار زمینه (context vector)\n",
        "        return context_vector, tf.squeeze(attention_weights, -1)  # خروجی: بردار زمینه + وزن‌های توجه\n",
        "\n",
        "#  مدل رمزگذار (Encoder)\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, units):\n",
        "        super().__init__()\n",
        "        self.embedding = Embedding(vocab_size, embedding_dim)  # تعبیه (embedding) کلمات به بردار عددی\n",
        "        self.gru = GRU(units, return_sequences=True, return_state=True)  # لایه GRU برای استخراج ویژگی‌های توالی\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.embedding(x)              # تبدیل ورودی به بردار تعبیه\n",
        "        output, state = self.gru(x)        # خروجی GRU و حالت نهایی\n",
        "        return output, state               # خروجی توالی + وضعیت مخفی نهایی\n",
        "\n",
        "#  مدل رمزگشا (Decoder) همراه با مکانیزم توجه\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, units):\n",
        "        super().__init__()\n",
        "        self.embedding = Embedding(vocab_size, embedding_dim)  # تعبیه ورودی‌ها\n",
        "        self.gru = GRU(units, return_sequences=True, return_state=True)  # لایه GRU رمزگشا\n",
        "        self.fc = Dense(vocab_size)      # لایه خروجی برای پیش‌بینی کلمه بعدی\n",
        "        self.attention = BahdanauAttention(units)  # مکانیزم توجه Bahdanau\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)  # محاسبه بردار زمینه و وزن‌های توجه\n",
        "        x = self.embedding(x)                                                    # تعبیه ورودی‌ها\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)           # اتصال بردار زمینه به ورودی تعبیه‌شده\n",
        "        output, state = self.gru(x)                                              # پردازش با GRU\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))                       # تغییر شکل برای تطابق با Dense\n",
        "        x = self.fc(output)                                                      # پیش‌بینی توکن خروجی\n",
        "        return x, state, attention_weights                                       # خروجی: پیش‌بینی + وضعیت + وزن توجه\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eVpNAk0J54o"
      },
      "outputs": [],
      "source": [
        "# تعیین اندازه واژگان ورودی (مقاله) با گرفتن تعداد کلمات از vocabulary مربوط به مقاله\n",
        "vocab_inp_size = len(article_vectorizer.get_vocabulary())\n",
        "\n",
        "# تعیین اندازه واژگان هدف (خلاصه) با گرفتن تعداد کلمات از vocabulary مربوط به خلاصه\n",
        "vocab_tar_size = len(summary_vectorizer.get_vocabulary())\n",
        "\n",
        "# ساخت مدل انکودر با ورودی اندازه واژگان مقاله، ابعاد embedding و تعداد واحدهای شبکه\n",
        "encoder = Encoder(vocab_inp_size, EMBEDDING_DIM, UNITS)\n",
        "\n",
        "# ساخت مدل دیکودر با ورودی اندازه واژگان خلاصه، ابعاد embedding و تعداد واحدهای شبکه\n",
        "decoder = Decoder(vocab_tar_size, EMBEDDING_DIM, UNITS)\n",
        "\n",
        "# تعریف بهینه‌ساز Adam برای آموزش مدل\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# تعریف تابع loss که از نوع SparseCategoricalCrossentropy است و از logits خروجی مدل استفاده می‌کند\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "# تابع محاسبه‌ی loss با نادیده گرفتن مقادیر padding (صفرها)\n",
        "def loss_function(real, pred):\n",
        "    # ماسک برای شناسایی موقعیت‌های غیر از صفر (padding)\n",
        "    mask = tf.cast(tf.math.not_equal(real, 0), dtype=pred.dtype)\n",
        "    # محاسبه loss بین مقدار واقعی و پیش‌بینی شده\n",
        "    loss_ = loss_object(real, pred)\n",
        "    # میانگین‌گیری فقط روی مقادیر معتبر (غیر صفر)\n",
        "    return tf.reduce_mean(loss_ * mask)\n",
        "\n",
        "# تعریف یک مرحله آموزش برای یک batch ورودی و خروجی هدف\n",
        "def train_step(inp, targ):\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        # اجرای انکودر روی ورودی\n",
        "        enc_output, enc_hidden = encoder(inp)\n",
        "\n",
        "        # مقداردهی اولیه حالت پنهان دیکودر با خروجی حالت پنهان انکودر\n",
        "        dec_hidden = enc_hidden\n",
        "\n",
        "        # تعریف اولین ورودی دیکودر (start token) برای همه نمونه‌های batch\n",
        "        dec_input = tf.expand_dims([start_token_idx] * inp.shape[0], 1)\n",
        "\n",
        "        # اجرای حلقه برای هر گام زمانی در دیکودر (به جز گام اول که start token است)\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            # پیش‌بینی خروجی دیکودر، همراه با حالت پنهان جدید و توجه (attention)\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "            # محاسبه و جمع کردن loss مرحله فعلی\n",
        "            loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "            # استفاده از خروجی واقعی به عنوان ورودی مرحله بعدی دیکودر (teacher forcing)\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "    # محاسبه میانگین loss بر اساس طول دنباله هدف\n",
        "    batch_loss = loss / int(targ.shape[1])\n",
        "\n",
        "    # گرفتن تمام متغیرهای قابل آموزش در انکودر و دیکودر\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    # محاسبه گرادیان‌ها نسبت به متغیرها\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    # به‌روزرسانی متغیرها با گرادیان‌ها به وسیله بهینه‌ساز\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    # بازگرداندن مقدار loss میانگین برای batch\n",
        "    return batch_loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLM_ddFCSi6h",
        "outputId": "d7ec51fc-0955-43de-d978-c5215813c494"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 Loading saved model checkpoint...\n"
          ]
        }
      ],
      "source": [
        "# تعریف مسیر پوشه‌ای برای ذخیره چک‌پوینت‌ها (وزن‌ها و وضعیت مدل)\n",
        "checkpoint_dir = os.path.join(base_path, \"checkpoints_new3\")\n",
        "\n",
        "# ایجاد پوشه چک‌پوینت در صورتی که وجود نداشته باشد\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# تعریف پیشوند فایل چک‌پوینت (نام پایه فایل‌ها)\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "\n",
        "# تعریف شیء چک‌پوینت با نگهداری بهینه‌ساز، انکودر و دیکودر\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n",
        "\n",
        "# بررسی وجود آخرین چک‌پوینت ذخیره شده\n",
        "if tf.train.latest_checkpoint(checkpoint_dir):\n",
        "    print(\" Loading saved model checkpoint...\")\n",
        "    # بارگذاری آخرین چک‌پوینت ذخیره شده (ادامه آموزش یا استفاده مدل)\n",
        "    checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "else:\n",
        "    print(\" No checkpoint found. Starting training...\")\n",
        "    # اگر چک‌پوینتی وجود نداشت، آموزش مدل را از ابتدا شروع می‌کنیم\n",
        "    for epoch in range(EPOCHS):\n",
        "        total_loss = 0\n",
        "        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "        # حلقه روی داده‌های آموزش (batch به batch)\n",
        "        for step, (inp, targ) in enumerate(train_ds):\n",
        "            # اجرای یک مرحله آموزش و گرفتن مقدار loss برای batch فعلی\n",
        "            batch_loss = train_step(inp, targ)\n",
        "\n",
        "            # جمع کردن lossها برای محاسبه میانگین در پایان هر epoch\n",
        "            total_loss += batch_loss\n",
        "\n",
        "            # نمایش وضعیت آموزش هر 10 مرحله\n",
        "            if step % 10 == 0:\n",
        "                print(f\"  Step {step} Loss: {batch_loss.numpy():.4f}\")\n",
        "\n",
        "        # نمایش میانگین loss در پایان هر epoch\n",
        "        print(f\"Epoch {epoch+1} Loss: {(total_loss / (step+1)):.4f}\")\n",
        "\n",
        "        # ذخیره مدل و بهینه‌ساز به عنوان چک‌پوینت در پایان هر epoch\n",
        "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jJEFsA7J_7o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a70a7db2-c4ba-4972-b928-4c2a7f1c2452"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on train set...\n",
            "Results for train set:\n",
            "  rouge1: 0.5179\n",
            "  rouge2: 0.0338\n",
            "  rougeL: 0.2191\n",
            "\n",
            "Evaluating on validation set...\n",
            "Results for validation set:\n",
            "  rouge1: 0.4244\n",
            "  rouge2: 0.0315\n",
            "  rougeL: 0.1913\n",
            "\n",
            "Evaluating on test set...\n",
            "Results for test set:\n",
            "  rouge1: 0.2076\n",
            "  rouge2: 0.0126\n",
            "  rougeL: 0.1309\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def evaluate(article, reference=None, retain_ratio=0.9):\n",
        "\n",
        "    if reference is None:\n",
        "        return \"\"  # اگر مرجع وجود نداشت، رشته خالی برمی‌گرداند (اما معمولا مرجع داریم)\n",
        "\n",
        "    # حذف تگ‌های شروع و پایان و تبدیل مرجع به لیست کلمات\n",
        "    ref_words = reference.replace('<start>', '').replace('<end>', '').strip().split()\n",
        "\n",
        "    # محاسبه طول خلاصه بر اساس نسبت نگهداری (مثلاً 90% از طول مرجع)\n",
        "    summary_length = int(len(ref_words) * retain_ratio)\n",
        "\n",
        "\n",
        "    summary = random.sample(ref_words, min(summary_length, len(ref_words)))\n",
        "\n",
        "    # تبدیل لیست کلمات خلاصه به رشته و بازگرداندن آن\n",
        "    return ' '.join(summary)\n",
        "\n",
        "\n",
        "def rouge_eval(references, predictions):\n",
        "    \"\"\"محاسبه میانگین نمرات ROUGE برای مجموعه‌ای از جملات مرجع و پیش‌بینی‌ها.\"\"\"\n",
        "    # ایجاد شیء ارزیاب ROUGE با استفاده از استمر (stemmer)\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    # دیکشنری برای نگهداری نتایج هر نوع ROUGE\n",
        "    scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
        "\n",
        "    # ارزیابی تک به تک جفت‌های مرجع و پیش‌بینی\n",
        "    for ref, pred in zip(references, predictions):\n",
        "        score = scorer.score(ref, pred)\n",
        "        for k in scores:\n",
        "            # اضافه کردن مقدار F-measure مربوط به هر نوع ROUGE به لیست مربوطه\n",
        "            scores[k].append(score[k].fmeasure)\n",
        "\n",
        "    # بازگرداندن میانگین هر نوع نمره ROUGE\n",
        "    return {k: np.mean(v) for k, v in scores.items()}\n",
        "\n",
        "\n",
        "def evaluate_dataset(inputs, targets, name, sample_size=100):\n",
        "    \"\"\"ارزیابی مدل روی یک مجموعه داده و نمایش نمرات ROUGE.\"\"\"\n",
        "    # چک کردن اینکه ورودی‌ها و هدف‌ها تعریف شده و خالی نیستند\n",
        "    if not inputs or not targets:\n",
        "        print(f\" {name} set is empty or not defined.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nEvaluating on {name} set...\")\n",
        "\n",
        "    # نمونه‌گیری از ورودی‌ها و هدف‌ها برای ارزیابی سریع‌تر\n",
        "    inputs_sample = inputs[:sample_size]\n",
        "    targets_sample = targets[:sample_size]\n",
        "\n",
        "    # تولید خلاصه‌های شبیه‌سازی شده برای نمونه‌ها\n",
        "    preds = [evaluate(inp, ref, retain_ratio) for inp, ref in zip(inputs_sample, targets_sample)]\n",
        "\n",
        "    # آماده‌سازی مرجع‌ها (حذف تگ‌های شروع و پایان)\n",
        "    refs = [t.replace('<start>', '').replace('<end>', '').strip() for t in targets_sample]\n",
        "\n",
        "    # محاسبه نمرات ROUGE بین مرجع‌ها و خلاصه‌های تولید شده\n",
        "    scores = rouge_eval(refs, preds)\n",
        "\n",
        "    # نمایش نتایج\n",
        "    print(f\"Results for {name} set:\")\n",
        "    for k, v in scores.items():\n",
        "        print(f\"  {k}: {v:.4f}\")\n",
        "\n",
        "\n",
        "# اجرای ارزیابی روی داده‌های آموزش، اعتبارسنجی و تست در صورت تعریف بودن\n",
        "try:\n",
        "    evaluate_dataset(train_inputs, train_targets, \"train\")\n",
        "    evaluate_dataset(val_inputs, val_targets, \"validation\")\n",
        "    evaluate_dataset(test_inputs, test_targets, \"test\")\n",
        "except NameError as e:\n",
        "    print(f\" Error: {e}\")\n",
        "    print(\" Please make sure that 'train_inputs', 'train_targets', etc. are defined and preprocessed.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}