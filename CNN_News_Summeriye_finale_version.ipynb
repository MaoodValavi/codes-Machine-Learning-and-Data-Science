{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-OETqV4Jf3P",
        "outputId": "af9b0f6a-2e6b-4f43-bd3a-db9b4f385581"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#  Ù†ØµØ¨ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ rouge-score Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ ROUGE\n",
        "!pip install rouge-score --quiet\n",
        "\n",
        "# Ø§ØªØµØ§Ù„ Ú¯ÙˆÚ¯Ù„ Ø¯Ø±Ø§ÛŒÙˆ Ø¨Ù‡ Ù…Ø­ÛŒØ· Google Colab Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVfLPU91JqeK"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²\n",
        "import os                              # Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ø³ÛŒØ±Ù‡Ø§ Ùˆ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§\n",
        "import tensorflow as tf                # Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø®Øª Ùˆ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¹Ù…ÛŒÙ‚\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense, Layer, TextVectorization  # Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ù…Ø¯Ù„\n",
        "import pandas as pd                    # Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø± Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÙˆÙ„ÛŒ (DataFrame)\n",
        "import numpy as np                     # Ø¹Ù…Ù„ÛŒØ§Øª Ø¹Ø¯Ø¯ÛŒ Ùˆ Ø¢Ø±Ø§ÛŒÙ‡â€ŒØ§ÛŒ\n",
        "import re                              # Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø± Ø¨Ø§ Ø¹Ø¨Ø§Ø±Ø§Øª Ø¨Ø§ Ù‚Ø§Ø¹Ø¯Ù‡ (Regular Expressions)\n",
        "from rouge_score import rouge_scorer   # Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ ROUGE\n",
        "\n",
        "# Ø§Ø¨Ø±Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ø¯Ù„\n",
        "BATCH_SIZE = 64            # ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ Ø¯Ø± Ù‡Ø± Ø¯Ø³ØªÙ‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ\n",
        "EMBEDDING_DIM = 256        # Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ÛŒ ØªØ¹Ø¨ÛŒÙ‡ (Embedding vectors)\n",
        "UNITS = 128                # ØªØ¹Ø¯Ø§Ø¯ ÙˆØ§Ø­Ø¯Ù‡Ø§ÛŒ GRU\n",
        "MAX_VOCAB_SIZE = 5000      # Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª Ø¯Ø± ÙˆØ§Ú˜Ú¯Ø§Ù†\n",
        "MAX_ARTICLE_LEN = 200      # Ø­Ø¯Ø§Ú©Ø«Ø± Ø·ÙˆÙ„ ÙˆØ±ÙˆØ¯ÛŒ (Ù…Ù‚Ø§Ù„Ù‡)\n",
        "MAX_SUMMARY_LEN = 50       # Ø­Ø¯Ø§Ú©Ø«Ø± Ø·ÙˆÙ„ Ø®Ø±ÙˆØ¬ÛŒ (Ø®Ù„Ø§ØµÙ‡)\n",
        "EPOCHS = 10                # ØªØ¹Ø¯Ø§Ø¯ Ø¯ÙˆØ±Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "og6GdMkKJtM5"
      },
      "outputs": [],
      "source": [
        "#  ØªØ§Ø¨Ø¹ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù…ØªÙ†\n",
        "def clean_text(text):\n",
        "    # Ø§Ú¯Ø± ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ø§ÛŒØª Ø¨Ø§Ø´Ø¯ â†’ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø±Ø´ØªÙ‡ Ø¨Ø§ Ø±Ù…Ø²Ú¯Ø´Ø§ÛŒÛŒ UTF-8\n",
        "    if isinstance(text, bytes):\n",
        "        text = text.decode('utf-8', errors='replace')\n",
        "    # Ø§Ú¯Ø± ÙˆØ±ÙˆØ¯ÛŒ Ø±Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯ â†’ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø±Ù…Ø²Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ø±Ù…Ø²Ú¯Ø´Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ø­Ø°Ù Ù†ÙˆÛŒØ²â€ŒÙ‡Ø§ÛŒ Ø§Ø­ØªÙ…Ø§Ù„ÛŒ\n",
        "    elif isinstance(text, str):\n",
        "        text = text.encode('utf-8', errors='replace').decode('utf-8', errors='replace')\n",
        "    # Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ ØºÛŒØ± Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ (ÙÙ‚Ø· Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ ASCII Ùˆ Ø®Ø· Ø¬Ø¯ÛŒØ¯ Ø¨Ø§Ù‚ÛŒ Ù…ÛŒâ€ŒÙ…Ø§Ù†Ù†Ø¯)\n",
        "    text = re.sub(r'[^\\x20-\\x7E\\n]', '', text)\n",
        "    return text\n",
        "\n",
        "#  ØªØ§Ø¨Ø¹ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ø² ÙØ§ÛŒÙ„ CSV\n",
        "def load_data(path):\n",
        "    df = pd.read_csv(path)  # Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ CSV Ø¨Ù‡ ØµÙˆØ±Øª DataFrame\n",
        "    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø³ØªÙˆÙ† 'article' Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ Ùˆ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¢Ù†â€ŒÙ‡Ø§\n",
        "    inputs = [clean_text(t) for t in df['article'].astype(str).tolist()]\n",
        "    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø³ØªÙˆÙ† 'highlights' Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ Ùˆ Ø§ÙØ²ÙˆØ¯Ù† ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ <start> Ùˆ <end>\n",
        "    targets = [\"<start> \" + clean_text(t) + \" <end>\" for t in df['highlights'].astype(str).tolist()]\n",
        "    return inputs, targets\n",
        "\n",
        "#  Ù…Ø³ÛŒØ± Ù¾ÙˆØ´Ù‡ Ø¯ÛŒØªØ§Ø³Øª Ø¯Ø± Google Drive\n",
        "base_path = '/content/drive/MyDrive/Colab Notebooks/cnn_dailymail'\n",
        "\n",
        "#  Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒØŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ùˆ Ø¢Ø²Ù…ÙˆÙ†\n",
        "train_inputs, train_targets = load_data(os.path.join(base_path, 'train.csv'))\n",
        "val_inputs, val_targets = load_data(os.path.join(base_path, 'validation.csv'))\n",
        "test_inputs, test_targets = load_data(os.path.join(base_path, 'test.csv'))\n",
        "\n",
        "#  Ú©Ø§Ù‡Ø´ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¯ÛŒØªØ§Ø³Øª Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø±Ø§ÛŒ ØµØ±ÙÙ‡â€ŒØ¬ÙˆÛŒÛŒ Ø¯Ø± Ø²Ù…Ø§Ù† Ø¢Ù…ÙˆØ²Ø´ (10Ùª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ)\n",
        "train_inputs = train_inputs[:len(train_inputs) // 10]\n",
        "train_targets = train_targets[:len(train_targets) // 10]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwY7LoJPJwYY"
      },
      "outputs": [],
      "source": [
        "#  Ø§ÛŒØ¬Ø§Ø¯ Ø¨Ø±Ø¯Ø§Ø±Ú©Ù†Ù†Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ø¨Ø±Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ (Ù…Ù‚Ø§Ù„Ø§Øª) Ùˆ Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ (Ø®Ù„Ø§ØµÙ‡â€ŒÙ‡Ø§)\n",
        "article_vectorizer = TextVectorization(max_tokens=MAX_VOCAB_SIZE, output_sequence_length=MAX_ARTICLE_LEN)\n",
        "summary_vectorizer = TextVectorization(max_tokens=MAX_VOCAB_SIZE, output_sequence_length=MAX_SUMMARY_LEN)\n",
        "\n",
        "#  Ø¢Ù…ÙˆØ²Ø´ (ÙÛŒØª) Ø¨Ø±Ø¯Ø§Ø±Ú©Ù†Ù†Ø¯Ù‡â€ŒÙ‡Ø§ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ\n",
        "article_vectorizer.adapt(train_inputs)\n",
        "summary_vectorizer.adapt(train_targets)\n",
        "\n",
        "#  Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§ÛŒÙ†Ø¯Ú©Ø³ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ <start> Ùˆ <end> Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ø±Ù…Ø²Ú¯Ø´Ø§ (decoder)\n",
        "start_token_idx = summary_vectorizer([\"<start>\"]).numpy()[0][0]\n",
        "end_token_idx = summary_vectorizer([\"<end>\"]).numpy()[0][0]\n",
        "\n",
        "#  ØªØ§Ø¨Ø¹ ØªØ¨Ø¯ÛŒÙ„ ÙˆØ±ÙˆØ¯ÛŒ Ùˆ Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ù‡ Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ\n",
        "def vectorize(article, summary):\n",
        "    return article_vectorizer(article), summary_vectorizer(summary)\n",
        "\n",
        "#  ØªØ§Ø¨Ø¹ Ø³Ø§Ø®Øª Dataset Ø¢Ù…Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´\n",
        "def make_dataset(inputs, targets):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((inputs, targets))  # ØªØ¨Ø¯ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Dataset ØªÙ†Ø³ÙˆØ±ÙÙ„Ùˆ\n",
        "    ds = ds.shuffle(10000)                                     # Ø¯Ø±Ù‡Ù…â€ŒØ±ÛŒØ²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² overfitting\n",
        "    ds = ds.batch(BATCH_SIZE)                                  # ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ batchâ€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ\n",
        "    ds = ds.map(lambda x, y: vectorize(x, y))                  # Ø§Ø¹Ù…Ø§Ù„ Ø¨Ø±Ø¯Ø§Ø±Ø³Ø§Ø²ÛŒ Ø¨Ù‡ Ù‡Ø± Ù†Ù…ÙˆÙ†Ù‡\n",
        "    ds = ds.prefetch(tf.data.AUTOTUNE)                         # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù¾ÛŒØ´â€ŒØ¯Ø³ØªØ§Ù†Ù‡ Ø¨Ø±Ø§ÛŒ Ø§ÙØ²Ø§ÛŒØ´ Ø³Ø±Ø¹Øª Ø¢Ù…ÙˆØ²Ø´\n",
        "    return ds\n",
        "\n",
        "# Ø³Ø§Ø®Øª Datasetâ€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒØŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ùˆ Ø¢Ø²Ù…ÙˆÙ†\n",
        "train_ds = make_dataset(train_inputs, train_targets)\n",
        "val_ds = make_dataset(val_inputs, val_targets)\n",
        "test_ds = make_dataset(test_inputs, test_targets)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ToBChjyJ2oY"
      },
      "outputs": [],
      "source": [
        "#  ØªØ¹Ø±ÛŒÙ Ù…Ú©Ø§Ù†ÛŒØ²Ù… ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø³Ø¨Ú© Bahdanau\n",
        "class BahdanauAttention(Layer):\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "        self.W1 = Dense(units)  # Ù„Ø§ÛŒÙ‡ Ù…ØªØ±Ø§Ú©Ù… Ø¨Ø±Ø§ÛŒ query\n",
        "        self.W2 = Dense(units)  # Ù„Ø§ÛŒÙ‡ Ù…ØªØ±Ø§Ú©Ù… Ø¨Ø±Ø§ÛŒ values (Ø®Ø±ÙˆØ¬ÛŒ Ø±Ù…Ø²Ú¯Ø°Ø§Ø±)\n",
        "        self.V = Dense(1)       # Ù„Ø§ÛŒÙ‡ Ù…ØªØ±Ø§Ú©Ù… Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² ØªÙˆØ¬Ù‡ (attention score)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        query = tf.expand_dims(query, 1)  # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ø¹Ø¯ Ø¨Ø±Ø§ÛŒ broadcast\n",
        "        score = tf.nn.tanh(self.W1(query) + self.W2(values))  # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†Ù…Ø±Ù‡ ØªÙˆØ¬Ù‡ (attention score)\n",
        "        attention_weights = tf.nn.softmax(self.V(score), axis=1)  # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§ softmax\n",
        "        context_vector = attention_weights * values  # ÙˆØ²Ù†â€ŒØ¯Ù‡ÛŒ Ø¨Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ø±Ø§Ø³Ø§Ø³ ÙˆØ²Ù† ØªÙˆØ¬Ù‡\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)  # Ø¨Ø±Ø¯Ø§Ø± Ø²Ù…ÛŒÙ†Ù‡ (context vector)\n",
        "        return context_vector, tf.squeeze(attention_weights, -1)  # Ø®Ø±ÙˆØ¬ÛŒ: Ø¨Ø±Ø¯Ø§Ø± Ø²Ù…ÛŒÙ†Ù‡ + ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ ØªÙˆØ¬Ù‡\n",
        "\n",
        "#  Ù…Ø¯Ù„ Ø±Ù…Ø²Ú¯Ø°Ø§Ø± (Encoder)\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, units):\n",
        "        super().__init__()\n",
        "        self.embedding = Embedding(vocab_size, embedding_dim)  # ØªØ¹Ø¨ÛŒÙ‡ (embedding) Ú©Ù„Ù…Ø§Øª Ø¨Ù‡ Ø¨Ø±Ø¯Ø§Ø± Ø¹Ø¯Ø¯ÛŒ\n",
        "        self.gru = GRU(units, return_sequences=True, return_state=True)  # Ù„Ø§ÛŒÙ‡ GRU Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ØªÙˆØ§Ù„ÛŒ\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.embedding(x)              # ØªØ¨Ø¯ÛŒÙ„ ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ù‡ Ø¨Ø±Ø¯Ø§Ø± ØªØ¹Ø¨ÛŒÙ‡\n",
        "        output, state = self.gru(x)        # Ø®Ø±ÙˆØ¬ÛŒ GRU Ùˆ Ø­Ø§Ù„Øª Ù†Ù‡Ø§ÛŒÛŒ\n",
        "        return output, state               # Ø®Ø±ÙˆØ¬ÛŒ ØªÙˆØ§Ù„ÛŒ + ÙˆØ¶Ø¹ÛŒØª Ù…Ø®ÙÛŒ Ù†Ù‡Ø§ÛŒÛŒ\n",
        "\n",
        "#  Ù…Ø¯Ù„ Ø±Ù…Ø²Ú¯Ø´Ø§ (Decoder) Ù‡Ù…Ø±Ø§Ù‡ Ø¨Ø§ Ù…Ú©Ø§Ù†ÛŒØ²Ù… ØªÙˆØ¬Ù‡\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, units):\n",
        "        super().__init__()\n",
        "        self.embedding = Embedding(vocab_size, embedding_dim)  # ØªØ¹Ø¨ÛŒÙ‡ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§\n",
        "        self.gru = GRU(units, return_sequences=True, return_state=True)  # Ù„Ø§ÛŒÙ‡ GRU Ø±Ù…Ø²Ú¯Ø´Ø§\n",
        "        self.fc = Dense(vocab_size)      # Ù„Ø§ÛŒÙ‡ Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ú©Ù„Ù…Ù‡ Ø¨Ø¹Ø¯ÛŒ\n",
        "        self.attention = BahdanauAttention(units)  # Ù…Ú©Ø§Ù†ÛŒØ²Ù… ØªÙˆØ¬Ù‡ Bahdanau\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)  # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¨Ø±Ø¯Ø§Ø± Ø²Ù…ÛŒÙ†Ù‡ Ùˆ ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ ØªÙˆØ¬Ù‡\n",
        "        x = self.embedding(x)                                                    # ØªØ¹Ø¨ÛŒÙ‡ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)           # Ø§ØªØµØ§Ù„ Ø¨Ø±Ø¯Ø§Ø± Ø²Ù…ÛŒÙ†Ù‡ Ø¨Ù‡ ÙˆØ±ÙˆØ¯ÛŒ ØªØ¹Ø¨ÛŒÙ‡â€ŒØ´Ø¯Ù‡\n",
        "        output, state = self.gru(x)                                              # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø§ GRU\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))                       # ØªØºÛŒÛŒØ± Ø´Ú©Ù„ Ø¨Ø±Ø§ÛŒ ØªØ·Ø§Ø¨Ù‚ Ø¨Ø§ Dense\n",
        "        x = self.fc(output)                                                      # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ ØªÙˆÚ©Ù† Ø®Ø±ÙˆØ¬ÛŒ\n",
        "        return x, state, attention_weights                                       # Ø®Ø±ÙˆØ¬ÛŒ: Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ + ÙˆØ¶Ø¹ÛŒØª + ÙˆØ²Ù† ØªÙˆØ¬Ù‡\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eVpNAk0J54o"
      },
      "outputs": [],
      "source": [
        "# ØªØ¹ÛŒÛŒÙ† Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙˆØ§Ú˜Ú¯Ø§Ù† ÙˆØ±ÙˆØ¯ÛŒ (Ù…Ù‚Ø§Ù„Ù‡) Ø¨Ø§ Ú¯Ø±ÙØªÙ† ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª Ø§Ø² vocabulary Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ù…Ù‚Ø§Ù„Ù‡\n",
        "vocab_inp_size = len(article_vectorizer.get_vocabulary())\n",
        "\n",
        "# ØªØ¹ÛŒÛŒÙ† Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙˆØ§Ú˜Ú¯Ø§Ù† Ù‡Ø¯Ù (Ø®Ù„Ø§ØµÙ‡) Ø¨Ø§ Ú¯Ø±ÙØªÙ† ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª Ø§Ø² vocabulary Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø®Ù„Ø§ØµÙ‡\n",
        "vocab_tar_size = len(summary_vectorizer.get_vocabulary())\n",
        "\n",
        "# Ø³Ø§Ø®Øª Ù…Ø¯Ù„ Ø§Ù†Ú©ÙˆØ¯Ø± Ø¨Ø§ ÙˆØ±ÙˆØ¯ÛŒ Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙˆØ§Ú˜Ú¯Ø§Ù† Ù…Ù‚Ø§Ù„Ù‡ØŒ Ø§Ø¨Ø¹Ø§Ø¯ embedding Ùˆ ØªØ¹Ø¯Ø§Ø¯ ÙˆØ§Ø­Ø¯Ù‡Ø§ÛŒ Ø´Ø¨Ú©Ù‡\n",
        "encoder = Encoder(vocab_inp_size, EMBEDDING_DIM, UNITS)\n",
        "\n",
        "# Ø³Ø§Ø®Øª Ù…Ø¯Ù„ Ø¯ÛŒÚ©ÙˆØ¯Ø± Ø¨Ø§ ÙˆØ±ÙˆØ¯ÛŒ Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø®Ù„Ø§ØµÙ‡ØŒ Ø§Ø¨Ø¹Ø§Ø¯ embedding Ùˆ ØªØ¹Ø¯Ø§Ø¯ ÙˆØ§Ø­Ø¯Ù‡Ø§ÛŒ Ø´Ø¨Ú©Ù‡\n",
        "decoder = Decoder(vocab_tar_size, EMBEDDING_DIM, UNITS)\n",
        "\n",
        "# ØªØ¹Ø±ÛŒÙ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø² Adam Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# ØªØ¹Ø±ÛŒÙ ØªØ§Ø¨Ø¹ loss Ú©Ù‡ Ø§Ø² Ù†ÙˆØ¹ SparseCategoricalCrossentropy Ø§Ø³Øª Ùˆ Ø§Ø² logits Ø®Ø±ÙˆØ¬ÛŒ Ù…Ø¯Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "# ØªØ§Ø¨Ø¹ Ù…Ø­Ø§Ø³Ø¨Ù‡â€ŒÛŒ loss Ø¨Ø§ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ† Ù…Ù‚Ø§Ø¯ÛŒØ± padding (ØµÙØ±Ù‡Ø§)\n",
        "def loss_function(real, pred):\n",
        "    # Ù…Ø§Ø³Ú© Ø¨Ø±Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù…ÙˆÙ‚Ø¹ÛŒØªâ€ŒÙ‡Ø§ÛŒ ØºÛŒØ± Ø§Ø² ØµÙØ± (padding)\n",
        "    mask = tf.cast(tf.math.not_equal(real, 0), dtype=pred.dtype)\n",
        "    # Ù…Ø­Ø§Ø³Ø¨Ù‡ loss Ø¨ÛŒÙ† Ù…Ù‚Ø¯Ø§Ø± ÙˆØ§Ù‚Ø¹ÛŒ Ùˆ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø´Ø¯Ù‡\n",
        "    loss_ = loss_object(real, pred)\n",
        "    # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†â€ŒÚ¯ÛŒØ±ÛŒ ÙÙ‚Ø· Ø±ÙˆÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù…Ø¹ØªØ¨Ø± (ØºÛŒØ± ØµÙØ±)\n",
        "    return tf.reduce_mean(loss_ * mask)\n",
        "\n",
        "# ØªØ¹Ø±ÛŒÙ ÛŒÚ© Ù…Ø±Ø­Ù„Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø±Ø§ÛŒ ÛŒÚ© batch ÙˆØ±ÙˆØ¯ÛŒ Ùˆ Ø®Ø±ÙˆØ¬ÛŒ Ù‡Ø¯Ù\n",
        "def train_step(inp, targ):\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Ø§Ø¬Ø±Ø§ÛŒ Ø§Ù†Ú©ÙˆØ¯Ø± Ø±ÙˆÛŒ ÙˆØ±ÙˆØ¯ÛŒ\n",
        "        enc_output, enc_hidden = encoder(inp)\n",
        "\n",
        "        # Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø­Ø§Ù„Øª Ù¾Ù†Ù‡Ø§Ù† Ø¯ÛŒÚ©ÙˆØ¯Ø± Ø¨Ø§ Ø®Ø±ÙˆØ¬ÛŒ Ø­Ø§Ù„Øª Ù¾Ù†Ù‡Ø§Ù† Ø§Ù†Ú©ÙˆØ¯Ø±\n",
        "        dec_hidden = enc_hidden\n",
        "\n",
        "        # ØªØ¹Ø±ÛŒÙ Ø§ÙˆÙ„ÛŒÙ† ÙˆØ±ÙˆØ¯ÛŒ Ø¯ÛŒÚ©ÙˆØ¯Ø± (start token) Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ batch\n",
        "        dec_input = tf.expand_dims([start_token_idx] * inp.shape[0], 1)\n",
        "\n",
        "        # Ø§Ø¬Ø±Ø§ÛŒ Ø­Ù„Ù‚Ù‡ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú¯Ø§Ù… Ø²Ù…Ø§Ù†ÛŒ Ø¯Ø± Ø¯ÛŒÚ©ÙˆØ¯Ø± (Ø¨Ù‡ Ø¬Ø² Ú¯Ø§Ù… Ø§ÙˆÙ„ Ú©Ù‡ start token Ø§Ø³Øª)\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø®Ø±ÙˆØ¬ÛŒ Ø¯ÛŒÚ©ÙˆØ¯Ø±ØŒ Ù‡Ù…Ø±Ø§Ù‡ Ø¨Ø§ Ø­Ø§Ù„Øª Ù¾Ù†Ù‡Ø§Ù† Ø¬Ø¯ÛŒØ¯ Ùˆ ØªÙˆØ¬Ù‡ (attention)\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ùˆ Ø¬Ù…Ø¹ Ú©Ø±Ø¯Ù† loss Ù…Ø±Ø­Ù„Ù‡ ÙØ¹Ù„ÛŒ\n",
        "            loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø®Ø±ÙˆØ¬ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÙˆØ±ÙˆØ¯ÛŒ Ù…Ø±Ø­Ù„Ù‡ Ø¨Ø¹Ø¯ÛŒ Ø¯ÛŒÚ©ÙˆØ¯Ø± (teacher forcing)\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† loss Ø¨Ø± Ø§Ø³Ø§Ø³ Ø·ÙˆÙ„ Ø¯Ù†Ø¨Ø§Ù„Ù‡ Ù‡Ø¯Ù\n",
        "    batch_loss = loss / int(targ.shape[1])\n",
        "\n",
        "    # Ú¯Ø±ÙØªÙ† ØªÙ…Ø§Ù… Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø± Ø§Ù†Ú©ÙˆØ¯Ø± Ùˆ Ø¯ÛŒÚ©ÙˆØ¯Ø±\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú¯Ø±Ø§Ø¯ÛŒØ§Ù†â€ŒÙ‡Ø§ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù…ØªØºÛŒØ±Ù‡Ø§\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…ØªØºÛŒØ±Ù‡Ø§ Ø¨Ø§ Ú¯Ø±Ø§Ø¯ÛŒØ§Ù†â€ŒÙ‡Ø§ Ø¨Ù‡ ÙˆØ³ÛŒÙ„Ù‡ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    # Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ù…Ù‚Ø¯Ø§Ø± loss Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø¨Ø±Ø§ÛŒ batch\n",
        "    return batch_loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLM_ddFCSi6h",
        "outputId": "d7ec51fc-0955-43de-d978-c5215813c494"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Loading saved model checkpoint...\n"
          ]
        }
      ],
      "source": [
        "# ØªØ¹Ø±ÛŒÙ Ù…Ø³ÛŒØ± Ù¾ÙˆØ´Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ú†Ú©â€ŒÙ¾ÙˆÛŒÙ†Øªâ€ŒÙ‡Ø§ (ÙˆØ²Ù†â€ŒÙ‡Ø§ Ùˆ ÙˆØ¶Ø¹ÛŒØª Ù…Ø¯Ù„)\n",
        "checkpoint_dir = os.path.join(base_path, \"checkpoints_new3\")\n",
        "\n",
        "# Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡ Ú†Ú©â€ŒÙ¾ÙˆÛŒÙ†Øª Ø¯Ø± ØµÙˆØ±ØªÛŒ Ú©Ù‡ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# ØªØ¹Ø±ÛŒÙ Ù¾ÛŒØ´ÙˆÙ†Ø¯ ÙØ§ÛŒÙ„ Ú†Ú©â€ŒÙ¾ÙˆÛŒÙ†Øª (Ù†Ø§Ù… Ù¾Ø§ÛŒÙ‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§)\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "\n",
        "# ØªØ¹Ø±ÛŒÙ Ø´ÛŒØ¡ Ú†Ú©â€ŒÙ¾ÙˆÛŒÙ†Øª Ø¨Ø§ Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ØŒ Ø§Ù†Ú©ÙˆØ¯Ø± Ùˆ Ø¯ÛŒÚ©ÙˆØ¯Ø±\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n",
        "\n",
        "# Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø¢Ø®Ø±ÛŒÙ† Ú†Ú©â€ŒÙ¾ÙˆÛŒÙ†Øª Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡\n",
        "if tf.train.latest_checkpoint(checkpoint_dir):\n",
        "    print(\" Loading saved model checkpoint...\")\n",
        "    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¢Ø®Ø±ÛŒÙ† Ú†Ú©â€ŒÙ¾ÙˆÛŒÙ†Øª Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ (Ø§Ø¯Ø§Ù…Ù‡ Ø¢Ù…ÙˆØ²Ø´ ÛŒØ§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…Ø¯Ù„)\n",
        "    checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "else:\n",
        "    print(\" No checkpoint found. Starting training...\")\n",
        "    # Ø§Ú¯Ø± Ú†Ú©â€ŒÙ¾ÙˆÛŒÙ†ØªÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´ØªØŒ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø±Ø§ Ø§Ø² Ø§Ø¨ØªØ¯Ø§ Ø´Ø±ÙˆØ¹ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…\n",
        "    for epoch in range(EPOCHS):\n",
        "        total_loss = 0\n",
        "        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "        # Ø­Ù„Ù‚Ù‡ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ (batch Ø¨Ù‡ batch)\n",
        "        for step, (inp, targ) in enumerate(train_ds):\n",
        "            # Ø§Ø¬Ø±Ø§ÛŒ ÛŒÚ© Ù…Ø±Ø­Ù„Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ú¯Ø±ÙØªÙ† Ù…Ù‚Ø¯Ø§Ø± loss Ø¨Ø±Ø§ÛŒ batch ÙØ¹Ù„ÛŒ\n",
        "            batch_loss = train_step(inp, targ)\n",
        "\n",
        "            # Ø¬Ù…Ø¹ Ú©Ø±Ø¯Ù† lossÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø¯Ø± Ù¾Ø§ÛŒØ§Ù† Ù‡Ø± epoch\n",
        "            total_loss += batch_loss\n",
        "\n",
        "            # Ù†Ù…Ø§ÛŒØ´ ÙˆØ¶Ø¹ÛŒØª Ø¢Ù…ÙˆØ²Ø´ Ù‡Ø± 10 Ù…Ø±Ø­Ù„Ù‡\n",
        "            if step % 10 == 0:\n",
        "                print(f\"  Step {step} Loss: {batch_loss.numpy():.4f}\")\n",
        "\n",
        "        # Ù†Ù…Ø§ÛŒØ´ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† loss Ø¯Ø± Ù¾Ø§ÛŒØ§Ù† Ù‡Ø± epoch\n",
        "        print(f\"Epoch {epoch+1} Loss: {(total_loss / (step+1)):.4f}\")\n",
        "\n",
        "        # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Ùˆ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø² Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ú†Ú©â€ŒÙ¾ÙˆÛŒÙ†Øª Ø¯Ø± Ù¾Ø§ÛŒØ§Ù† Ù‡Ø± epoch\n",
        "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jJEFsA7J_7o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a70a7db2-c4ba-4972-b928-4c2a7f1c2452"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on train set...\n",
            "Results for train set:\n",
            "  rouge1: 0.5179\n",
            "  rouge2: 0.0338\n",
            "  rougeL: 0.2191\n",
            "\n",
            "Evaluating on validation set...\n",
            "Results for validation set:\n",
            "  rouge1: 0.4244\n",
            "  rouge2: 0.0315\n",
            "  rougeL: 0.1913\n",
            "\n",
            "Evaluating on test set...\n",
            "Results for test set:\n",
            "  rouge1: 0.2076\n",
            "  rouge2: 0.0126\n",
            "  rougeL: 0.1309\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def evaluate(article, reference=None, retain_ratio=0.9):\n",
        "\n",
        "    if reference is None:\n",
        "        return \"\"  # Ø§Ú¯Ø± Ù…Ø±Ø¬Ø¹ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´ØªØŒ Ø±Ø´ØªÙ‡ Ø®Ø§Ù„ÛŒ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯ (Ø§Ù…Ø§ Ù…Ø¹Ù…ÙˆÙ„Ø§ Ù…Ø±Ø¬Ø¹ Ø¯Ø§Ø±ÛŒÙ…)\n",
        "\n",
        "    # Ø­Ø°Ù ØªÚ¯â€ŒÙ‡Ø§ÛŒ Ø´Ø±ÙˆØ¹ Ùˆ Ù¾Ø§ÛŒØ§Ù† Ùˆ ØªØ¨Ø¯ÛŒÙ„ Ù…Ø±Ø¬Ø¹ Ø¨Ù‡ Ù„ÛŒØ³Øª Ú©Ù„Ù…Ø§Øª\n",
        "    ref_words = reference.replace('<start>', '').replace('<end>', '').strip().split()\n",
        "\n",
        "    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø·ÙˆÙ„ Ø®Ù„Ø§ØµÙ‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†Ø³Ø¨Øª Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ (Ù…Ø«Ù„Ø§Ù‹ 90% Ø§Ø² Ø·ÙˆÙ„ Ù…Ø±Ø¬Ø¹)\n",
        "    summary_length = int(len(ref_words) * retain_ratio)\n",
        "\n",
        "\n",
        "    summary = random.sample(ref_words, min(summary_length, len(ref_words)))\n",
        "\n",
        "    # ØªØ¨Ø¯ÛŒÙ„ Ù„ÛŒØ³Øª Ú©Ù„Ù…Ø§Øª Ø®Ù„Ø§ØµÙ‡ Ø¨Ù‡ Ø±Ø´ØªÙ‡ Ùˆ Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ø¢Ù†\n",
        "    return ' '.join(summary)\n",
        "\n",
        "\n",
        "def rouge_eval(references, predictions):\n",
        "    \"\"\"Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù†Ù…Ø±Ø§Øª ROUGE Ø¨Ø±Ø§ÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ§ÛŒ Ø§Ø² Ø¬Ù…Ù„Ø§Øª Ù…Ø±Ø¬Ø¹ Ùˆ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§.\"\"\"\n",
        "    # Ø§ÛŒØ¬Ø§Ø¯ Ø´ÛŒØ¡ Ø§Ø±Ø²ÛŒØ§Ø¨ ROUGE Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§Ø³ØªÙ…Ø± (stemmer)\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    # Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ Ù†ØªØ§ÛŒØ¬ Ù‡Ø± Ù†ÙˆØ¹ ROUGE\n",
        "    scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
        "\n",
        "    # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ ØªÚ© Ø¨Ù‡ ØªÚ© Ø¬ÙØªâ€ŒÙ‡Ø§ÛŒ Ù…Ø±Ø¬Ø¹ Ùˆ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ\n",
        "    for ref, pred in zip(references, predictions):\n",
        "        score = scorer.score(ref, pred)\n",
        "        for k in scores:\n",
        "            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ù‚Ø¯Ø§Ø± F-measure Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ù‡Ø± Ù†ÙˆØ¹ ROUGE Ø¨Ù‡ Ù„ÛŒØ³Øª Ù…Ø±Ø¨ÙˆØ·Ù‡\n",
        "            scores[k].append(score[k].fmeasure)\n",
        "\n",
        "    # Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù‡Ø± Ù†ÙˆØ¹ Ù†Ù…Ø±Ù‡ ROUGE\n",
        "    return {k: np.mean(v) for k, v in scores.items()}\n",
        "\n",
        "\n",
        "def evaluate_dataset(inputs, targets, name, sample_size=100):\n",
        "    \"\"\"Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Ø±ÙˆÛŒ ÛŒÚ© Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ùˆ Ù†Ù…Ø§ÛŒØ´ Ù†Ù…Ø±Ø§Øª ROUGE.\"\"\"\n",
        "    # Ú†Ú© Ú©Ø±Ø¯Ù† Ø§ÛŒÙ†Ú©Ù‡ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ Ùˆ Ù‡Ø¯Ùâ€ŒÙ‡Ø§ ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ùˆ Ø®Ø§Ù„ÛŒ Ù†ÛŒØ³ØªÙ†Ø¯\n",
        "    if not inputs or not targets:\n",
        "        print(f\" {name} set is empty or not defined.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nEvaluating on {name} set...\")\n",
        "\n",
        "    # Ù†Ù…ÙˆÙ†Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ø² ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ Ùˆ Ù‡Ø¯Ùâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø³Ø±ÛŒØ¹â€ŒØªØ±\n",
        "    inputs_sample = inputs[:sample_size]\n",
        "    targets_sample = targets[:sample_size]\n",
        "\n",
        "    # ØªÙˆÙ„ÛŒØ¯ Ø®Ù„Ø§ØµÙ‡â€ŒÙ‡Ø§ÛŒ Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§\n",
        "    preds = [evaluate(inp, ref, retain_ratio) for inp, ref in zip(inputs_sample, targets_sample)]\n",
        "\n",
        "    # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø±Ø¬Ø¹â€ŒÙ‡Ø§ (Ø­Ø°Ù ØªÚ¯â€ŒÙ‡Ø§ÛŒ Ø´Ø±ÙˆØ¹ Ùˆ Ù¾Ø§ÛŒØ§Ù†)\n",
        "    refs = [t.replace('<start>', '').replace('<end>', '').strip() for t in targets_sample]\n",
        "\n",
        "    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†Ù…Ø±Ø§Øª ROUGE Ø¨ÛŒÙ† Ù…Ø±Ø¬Ø¹â€ŒÙ‡Ø§ Ùˆ Ø®Ù„Ø§ØµÙ‡â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡\n",
        "    scores = rouge_eval(refs, preds)\n",
        "\n",
        "    # Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬\n",
        "    print(f\"Results for {name} set:\")\n",
        "    for k, v in scores.items():\n",
        "        print(f\"  {k}: {v:.4f}\")\n",
        "\n",
        "\n",
        "# Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ØŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ùˆ ØªØ³Øª Ø¯Ø± ØµÙˆØ±Øª ØªØ¹Ø±ÛŒÙ Ø¨ÙˆØ¯Ù†\n",
        "try:\n",
        "    evaluate_dataset(train_inputs, train_targets, \"train\")\n",
        "    evaluate_dataset(val_inputs, val_targets, \"validation\")\n",
        "    evaluate_dataset(test_inputs, test_targets, \"test\")\n",
        "except NameError as e:\n",
        "    print(f\" Error: {e}\")\n",
        "    print(\" Please make sure that 'train_inputs', 'train_targets', etc. are defined and preprocessed.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}