{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c14bba58-c889-405c-9abf-1eeaaf1636d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "286a37e7-7017-4b04-9bb9-e6ee2936477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"roboBohr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be214fcc-0fb1-4a7f-aaa0-ebabed6245e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcopy=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0d5f073-2ac4-4669-90fe-f6d7aaa543e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>1267</th>\n",
       "      <th>1268</th>\n",
       "      <th>1269</th>\n",
       "      <th>1270</th>\n",
       "      <th>1271</th>\n",
       "      <th>1272</th>\n",
       "      <th>1273</th>\n",
       "      <th>1274</th>\n",
       "      <th>pubchem_id</th>\n",
       "      <th>Eat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>17.817765</td>\n",
       "      <td>12.469551</td>\n",
       "      <td>12.458130</td>\n",
       "      <td>12.454607</td>\n",
       "      <td>12.447345</td>\n",
       "      <td>12.433065</td>\n",
       "      <td>12.426926</td>\n",
       "      <td>12.387474</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25004</td>\n",
       "      <td>-19.013763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>20.649126</td>\n",
       "      <td>18.527789</td>\n",
       "      <td>17.891535</td>\n",
       "      <td>17.887995</td>\n",
       "      <td>17.871731</td>\n",
       "      <td>17.852586</td>\n",
       "      <td>17.729842</td>\n",
       "      <td>15.864270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25005</td>\n",
       "      <td>-10.161019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>17.830377</td>\n",
       "      <td>12.512263</td>\n",
       "      <td>12.404775</td>\n",
       "      <td>12.394493</td>\n",
       "      <td>12.391564</td>\n",
       "      <td>12.324461</td>\n",
       "      <td>12.238106</td>\n",
       "      <td>10.423249</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25006</td>\n",
       "      <td>-9.376619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>17.875810</td>\n",
       "      <td>17.871259</td>\n",
       "      <td>17.862402</td>\n",
       "      <td>17.850920</td>\n",
       "      <td>17.850440</td>\n",
       "      <td>12.558105</td>\n",
       "      <td>12.557645</td>\n",
       "      <td>12.517583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25009</td>\n",
       "      <td>-13.776438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>17.883818</td>\n",
       "      <td>17.868256</td>\n",
       "      <td>17.864221</td>\n",
       "      <td>17.818540</td>\n",
       "      <td>12.508657</td>\n",
       "      <td>12.490519</td>\n",
       "      <td>12.450098</td>\n",
       "      <td>10.597068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25011</td>\n",
       "      <td>-8.537140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1278 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0          0          1          2          3          4  \\\n",
       "0           0  73.516695  17.817765  12.469551  12.458130  12.454607   \n",
       "1           1  73.516695  20.649126  18.527789  17.891535  17.887995   \n",
       "2           2  73.516695  17.830377  12.512263  12.404775  12.394493   \n",
       "3           3  73.516695  17.875810  17.871259  17.862402  17.850920   \n",
       "4           4  73.516695  17.883818  17.868256  17.864221  17.818540   \n",
       "\n",
       "           5          6          7          8  ...  1267  1268  1269  1270  \\\n",
       "0  12.447345  12.433065  12.426926  12.387474  ...   0.0   0.0   0.5   0.0   \n",
       "1  17.871731  17.852586  17.729842  15.864270  ...   0.0   0.0   0.0   0.0   \n",
       "2  12.391564  12.324461  12.238106  10.423249  ...   0.0   0.0   0.0   0.0   \n",
       "3  17.850440  12.558105  12.557645  12.517583  ...   0.0   0.0   0.0   0.0   \n",
       "4  12.508657  12.490519  12.450098  10.597068  ...   0.0   0.0   0.0   0.0   \n",
       "\n",
       "   1271  1272  1273  1274  pubchem_id        Eat  \n",
       "0   0.0   0.0   0.0   0.0       25004 -19.013763  \n",
       "1   0.0   0.0   0.0   0.0       25005 -10.161019  \n",
       "2   0.0   0.0   0.0   0.0       25006  -9.376619  \n",
       "3   0.0   0.0   0.0   0.0       25009 -13.776438  \n",
       "4   0.0   0.0   0.0   0.0       25011  -8.537140  \n",
       "\n",
       "[5 rows x 1278 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfcopy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92523a45-60d4-439b-9f55-7ecced371835",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcopy.drop([\"0\",\"pubchem_id\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf4d4d04-6469-49a0-a3f6-9c446e148cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1266</th>\n",
       "      <th>1267</th>\n",
       "      <th>1268</th>\n",
       "      <th>1269</th>\n",
       "      <th>1270</th>\n",
       "      <th>1271</th>\n",
       "      <th>1272</th>\n",
       "      <th>1273</th>\n",
       "      <th>1274</th>\n",
       "      <th>Eat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17.817765</td>\n",
       "      <td>12.469551</td>\n",
       "      <td>12.458130</td>\n",
       "      <td>12.454607</td>\n",
       "      <td>12.447345</td>\n",
       "      <td>12.433065</td>\n",
       "      <td>12.426926</td>\n",
       "      <td>12.387474</td>\n",
       "      <td>12.365984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-19.013763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20.649126</td>\n",
       "      <td>18.527789</td>\n",
       "      <td>17.891535</td>\n",
       "      <td>17.887995</td>\n",
       "      <td>17.871731</td>\n",
       "      <td>17.852586</td>\n",
       "      <td>17.729842</td>\n",
       "      <td>15.864270</td>\n",
       "      <td>15.227643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.161019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>17.830377</td>\n",
       "      <td>12.512263</td>\n",
       "      <td>12.404775</td>\n",
       "      <td>12.394493</td>\n",
       "      <td>12.391564</td>\n",
       "      <td>12.324461</td>\n",
       "      <td>12.238106</td>\n",
       "      <td>10.423249</td>\n",
       "      <td>8.698826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9.376619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17.875810</td>\n",
       "      <td>17.871259</td>\n",
       "      <td>17.862402</td>\n",
       "      <td>17.850920</td>\n",
       "      <td>17.850440</td>\n",
       "      <td>12.558105</td>\n",
       "      <td>12.557645</td>\n",
       "      <td>12.517583</td>\n",
       "      <td>12.444141</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-13.776438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>17.883818</td>\n",
       "      <td>17.868256</td>\n",
       "      <td>17.864221</td>\n",
       "      <td>17.818540</td>\n",
       "      <td>12.508657</td>\n",
       "      <td>12.490519</td>\n",
       "      <td>12.450098</td>\n",
       "      <td>10.597068</td>\n",
       "      <td>10.595914</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.537140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1276 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0          1          2          3          4          5  \\\n",
       "0           0  17.817765  12.469551  12.458130  12.454607  12.447345   \n",
       "1           1  20.649126  18.527789  17.891535  17.887995  17.871731   \n",
       "2           2  17.830377  12.512263  12.404775  12.394493  12.391564   \n",
       "3           3  17.875810  17.871259  17.862402  17.850920  17.850440   \n",
       "4           4  17.883818  17.868256  17.864221  17.818540  12.508657   \n",
       "\n",
       "           6          7          8          9  ...  1266  1267  1268  1269  \\\n",
       "0  12.433065  12.426926  12.387474  12.365984  ...   0.0   0.0   0.0   0.5   \n",
       "1  17.852586  17.729842  15.864270  15.227643  ...   0.0   0.0   0.0   0.0   \n",
       "2  12.324461  12.238106  10.423249   8.698826  ...   0.0   0.0   0.0   0.0   \n",
       "3  12.558105  12.557645  12.517583  12.444141  ...   0.0   0.0   0.0   0.0   \n",
       "4  12.490519  12.450098  10.597068  10.595914  ...   0.0   0.0   0.0   0.0   \n",
       "\n",
       "   1270  1271  1272  1273  1274        Eat  \n",
       "0   0.0   0.0   0.0   0.0   0.0 -19.013763  \n",
       "1   0.0   0.0   0.0   0.0   0.0 -10.161019  \n",
       "2   0.0   0.0   0.0   0.0   0.0  -9.376619  \n",
       "3   0.0   0.0   0.0   0.0   0.0 -13.776438  \n",
       "4   0.0   0.0   0.0   0.0   0.0  -8.537140  \n",
       "\n",
       "[5 rows x 1276 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfcopy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84ad6cd7-305c-46cf-9ddc-1b8974c22da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcopy.drop([\"Unnamed: 0\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e782d36-cbd7-44aa-ae41-f97195f513af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>1266</th>\n",
       "      <th>1267</th>\n",
       "      <th>1268</th>\n",
       "      <th>1269</th>\n",
       "      <th>1270</th>\n",
       "      <th>1271</th>\n",
       "      <th>1272</th>\n",
       "      <th>1273</th>\n",
       "      <th>1274</th>\n",
       "      <th>Eat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.817765</td>\n",
       "      <td>12.469551</td>\n",
       "      <td>12.458130</td>\n",
       "      <td>12.454607</td>\n",
       "      <td>12.447345</td>\n",
       "      <td>12.433065</td>\n",
       "      <td>12.426926</td>\n",
       "      <td>12.387474</td>\n",
       "      <td>12.365984</td>\n",
       "      <td>12.342823</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-19.013763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.649126</td>\n",
       "      <td>18.527789</td>\n",
       "      <td>17.891535</td>\n",
       "      <td>17.887995</td>\n",
       "      <td>17.871731</td>\n",
       "      <td>17.852586</td>\n",
       "      <td>17.729842</td>\n",
       "      <td>15.864270</td>\n",
       "      <td>15.227643</td>\n",
       "      <td>15.202985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.161019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.830377</td>\n",
       "      <td>12.512263</td>\n",
       "      <td>12.404775</td>\n",
       "      <td>12.394493</td>\n",
       "      <td>12.391564</td>\n",
       "      <td>12.324461</td>\n",
       "      <td>12.238106</td>\n",
       "      <td>10.423249</td>\n",
       "      <td>8.698826</td>\n",
       "      <td>7.607776</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9.376619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.875810</td>\n",
       "      <td>17.871259</td>\n",
       "      <td>17.862402</td>\n",
       "      <td>17.850920</td>\n",
       "      <td>17.850440</td>\n",
       "      <td>12.558105</td>\n",
       "      <td>12.557645</td>\n",
       "      <td>12.517583</td>\n",
       "      <td>12.444141</td>\n",
       "      <td>12.420665</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-13.776438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.883818</td>\n",
       "      <td>17.868256</td>\n",
       "      <td>17.864221</td>\n",
       "      <td>17.818540</td>\n",
       "      <td>12.508657</td>\n",
       "      <td>12.490519</td>\n",
       "      <td>12.450098</td>\n",
       "      <td>10.597068</td>\n",
       "      <td>10.595914</td>\n",
       "      <td>10.485270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.537140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1275 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           1          2          3          4          5          6  \\\n",
       "0  17.817765  12.469551  12.458130  12.454607  12.447345  12.433065   \n",
       "1  20.649126  18.527789  17.891535  17.887995  17.871731  17.852586   \n",
       "2  17.830377  12.512263  12.404775  12.394493  12.391564  12.324461   \n",
       "3  17.875810  17.871259  17.862402  17.850920  17.850440  12.558105   \n",
       "4  17.883818  17.868256  17.864221  17.818540  12.508657  12.490519   \n",
       "\n",
       "           7          8          9         10  ...  1266  1267  1268  1269  \\\n",
       "0  12.426926  12.387474  12.365984  12.342823  ...   0.0   0.0   0.0   0.5   \n",
       "1  17.729842  15.864270  15.227643  15.202985  ...   0.0   0.0   0.0   0.0   \n",
       "2  12.238106  10.423249   8.698826   7.607776  ...   0.0   0.0   0.0   0.0   \n",
       "3  12.557645  12.517583  12.444141  12.420665  ...   0.0   0.0   0.0   0.0   \n",
       "4  12.450098  10.597068  10.595914  10.485270  ...   0.0   0.0   0.0   0.0   \n",
       "\n",
       "   1270  1271  1272  1273  1274        Eat  \n",
       "0   0.0   0.0   0.0   0.0   0.0 -19.013763  \n",
       "1   0.0   0.0   0.0   0.0   0.0 -10.161019  \n",
       "2   0.0   0.0   0.0   0.0   0.0  -9.376619  \n",
       "3   0.0   0.0   0.0   0.0   0.0 -13.776438  \n",
       "4   0.0   0.0   0.0   0.0   0.0  -8.537140  \n",
       "\n",
       "[5 rows x 1275 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfcopy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2fbbd5e2-d007-4dfc-912b-a1ced5e7eec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of columns: 528\n",
      "Reduced number of columns: 17\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming dfcopy_reduced is your DataFrame with 528 columns\n",
    "# Step 1: Calculate the correlation matrix\n",
    "correlation_matrix = dfcopy2.corr(method='pearson')\n",
    "\n",
    "# Step 2: Create a set to keep track of columns to drop\n",
    "columns_to_drop = set()\n",
    "\n",
    "# Step 3: Iterate over the correlation matrix\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.8:  # Check if the correlation is greater than 0.8\n",
    "            colname = correlation_matrix.columns[i]  # Get the column name\n",
    "            columns_to_drop.add(colname)  # Add it to the set of columns to drop\n",
    "\n",
    "# Optional: Choose to keep the first column from each pair\n",
    "# You could also implement a strategy to keep the column with a higher variance or some other criteria.\n",
    "\n",
    "# Step 4: Drop the identified columns from the original DataFrame\n",
    "dfcopy3 = dfcopy2.drop(columns=columns_to_drop)\n",
    "\n",
    "# Step 5: Display the shape of the new DataFrame\n",
    "print(f\"Original number of columns: {dfcopy2.shape[1]}\")\n",
    "print(f\"Reduced number of columns: {dfcopy3.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "891a52ce-a679-4242-84bf-a4ce96d005d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>50</th>\n",
       "      <th>99</th>\n",
       "      <th>147</th>\n",
       "      <th>194</th>\n",
       "      <th>240</th>\n",
       "      <th>285</th>\n",
       "      <th>329</th>\n",
       "      <th>840</th>\n",
       "      <th>869</th>\n",
       "      <th>897</th>\n",
       "      <th>924</th>\n",
       "      <th>950</th>\n",
       "      <th>999</th>\n",
       "      <th>1022</th>\n",
       "      <th>1065</th>\n",
       "      <th>1085</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.817765</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.649126</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>53.358707</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.830377</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.875810</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.883818</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           1         50         99        147        194        240  \\\n",
       "0  17.817765  36.858105  36.858105  36.858105  36.858105  36.858105   \n",
       "1  20.649126  73.516695  73.516695  73.516695  73.516695  53.358707   \n",
       "2  17.830377  36.858105  36.858105  36.858105  36.858105  36.858105   \n",
       "3  17.875810  73.516695  73.516695  36.858105  36.858105  36.858105   \n",
       "4  17.883818  73.516695  36.858105  36.858105  36.858105  36.858105   \n",
       "\n",
       "         285        329  840  869  897  924  950  999  1022  1065  1085  \n",
       "0  36.858105  36.858105  0.5  0.5  0.5  0.5  0.5  0.5   0.5   0.5   0.5  \n",
       "1  36.858105  36.858105  0.5  0.5  0.5  0.5  0.0  0.0   0.0   0.0   0.0  \n",
       "2  36.858105  36.858105  0.5  0.5  0.5  0.0  0.0  0.0   0.0   0.0   0.0  \n",
       "3  36.858105  36.858105  0.5  0.5  0.5  0.5  0.5  0.5   0.5   0.5   0.5  \n",
       "4  36.858105  36.858105  0.5  0.0  0.0  0.0  0.0  0.0   0.0   0.0   0.0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfcopy3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6321fb8f-d234-4efc-b960-ba4156f67e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16242, 18)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming dfcopy2 is your original DataFrame\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = dfcopy.corr()\n",
    "\n",
    "# Create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "# Create a list to hold columns to drop\n",
    "columns_to_drop = []\n",
    "\n",
    "# Iterate through the columns in the correlation matrix\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.8:  # Check for correlation > 0.8\n",
    "            colname = corr_matrix.columns[i]\n",
    "            if colname != 'Eat' and colname not in columns_to_drop:\n",
    "                columns_to_drop.append(colname)\n",
    "\n",
    "# Drop the columns from dfcopy2 and create dfcopy4\n",
    "dfcopy4 = dfcopy.drop(columns=columns_to_drop)\n",
    "\n",
    "# Show the new DataFrame shape\n",
    "print(dfcopy4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bef41133-fede-4d7c-a884-f74ec8bb6543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>50</th>\n",
       "      <th>99</th>\n",
       "      <th>147</th>\n",
       "      <th>194</th>\n",
       "      <th>240</th>\n",
       "      <th>285</th>\n",
       "      <th>329</th>\n",
       "      <th>840</th>\n",
       "      <th>869</th>\n",
       "      <th>897</th>\n",
       "      <th>924</th>\n",
       "      <th>950</th>\n",
       "      <th>999</th>\n",
       "      <th>1022</th>\n",
       "      <th>1065</th>\n",
       "      <th>1085</th>\n",
       "      <th>Eat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.817765</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-19.013763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.649126</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>53.358707</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.161019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.830377</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9.376619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.875810</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-13.776438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.883818</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.537140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           1         50         99        147        194        240  \\\n",
       "0  17.817765  36.858105  36.858105  36.858105  36.858105  36.858105   \n",
       "1  20.649126  73.516695  73.516695  73.516695  73.516695  53.358707   \n",
       "2  17.830377  36.858105  36.858105  36.858105  36.858105  36.858105   \n",
       "3  17.875810  73.516695  73.516695  36.858105  36.858105  36.858105   \n",
       "4  17.883818  73.516695  36.858105  36.858105  36.858105  36.858105   \n",
       "\n",
       "         285        329  840  869  897  924  950  999  1022  1065  1085  \\\n",
       "0  36.858105  36.858105  0.5  0.5  0.5  0.5  0.5  0.5   0.5   0.5   0.5   \n",
       "1  36.858105  36.858105  0.5  0.5  0.5  0.5  0.0  0.0   0.0   0.0   0.0   \n",
       "2  36.858105  36.858105  0.5  0.5  0.5  0.0  0.0  0.0   0.0   0.0   0.0   \n",
       "3  36.858105  36.858105  0.5  0.5  0.5  0.5  0.5  0.5   0.5   0.5   0.5   \n",
       "4  36.858105  36.858105  0.5  0.0  0.0  0.0  0.0  0.0   0.0   0.0   0.0   \n",
       "\n",
       "         Eat  \n",
       "0 -19.013763  \n",
       "1 -10.161019  \n",
       "2  -9.376619  \n",
       "3 -13.776438  \n",
       "4  -8.537140  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfcopy4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7d52531-454c-4c75-a0d9-f3c728afe8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          1        50        99       147       194       240       285  \\\n",
      "0 -0.534448 -0.581890 -0.591997 -0.523183 -0.418308 -0.212815  0.004856   \n",
      "1 -0.207476 -0.007514  0.553831  1.237657  2.177325  1.105210  0.004856   \n",
      "2 -0.532992 -0.581890 -0.591997 -0.523183 -0.418308 -0.212815  0.004856   \n",
      "3 -0.527745 -0.007514  0.553831 -0.523183 -0.418308 -0.212815  0.004856   \n",
      "4 -0.526820 -0.007514 -0.591997 -0.523183 -0.418308 -0.212815  0.004856   \n",
      "\n",
      "        329       840       869       897       924       950       999  \\\n",
      "0  0.185074 -0.140132 -0.086794 -0.036400  0.022466  0.083268  0.207589   \n",
      "1  0.185074 -0.140132 -0.086794 -0.036400  0.022466 -0.168848 -0.174526   \n",
      "2  0.185074 -0.140132 -0.086794 -0.036400 -0.167103 -0.168848 -0.174526   \n",
      "3  0.185074 -0.140132 -0.086794 -0.036400  0.022466  0.083268  0.207589   \n",
      "4  0.185074 -0.140132 -0.193003 -0.174403 -0.167103 -0.168848 -0.174526   \n",
      "\n",
      "       1022      1065      1085        Eat  \n",
      "0  0.319101  0.595678  1.093037 -19.013763  \n",
      "1 -0.189613 -0.216587 -0.315684 -10.161019  \n",
      "2 -0.189613 -0.216587 -0.315684  -9.376619  \n",
      "3  0.319101  0.595678  1.093037 -13.776438  \n",
      "4 -0.189613 -0.216587 -0.315684  -8.537140  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming dfcopy4 is your DataFrame\n",
    "\n",
    "# Step 1: Separate the 'Eat' column\n",
    "eat_column = dfcopy4[['Eat']]\n",
    "features_to_scale = dfcopy4.drop(columns=['Eat'])\n",
    "\n",
    "# Step 2: Create an instance of StandardScaler and fit/transform the features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features_to_scale)\n",
    "\n",
    "# Convert the scaled features back to a DataFrame\n",
    "scaled_df = pd.DataFrame(scaled_features, columns=features_to_scale.columns)\n",
    "\n",
    "# Step 3: Concatenate the scaled features with the 'Eat' column\n",
    "dfcopy5 = pd.concat([scaled_df, eat_column.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Show the first few rows of the new DataFrame\n",
    "print(dfcopy5.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "127e8f5b-08ca-4457-b74f-1f3ab08a9e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>50</th>\n",
       "      <th>99</th>\n",
       "      <th>147</th>\n",
       "      <th>194</th>\n",
       "      <th>240</th>\n",
       "      <th>285</th>\n",
       "      <th>329</th>\n",
       "      <th>840</th>\n",
       "      <th>869</th>\n",
       "      <th>897</th>\n",
       "      <th>924</th>\n",
       "      <th>950</th>\n",
       "      <th>999</th>\n",
       "      <th>1022</th>\n",
       "      <th>1065</th>\n",
       "      <th>1085</th>\n",
       "      <th>Eat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.534448</td>\n",
       "      <td>-0.581890</td>\n",
       "      <td>-0.591997</td>\n",
       "      <td>-0.523183</td>\n",
       "      <td>-0.418308</td>\n",
       "      <td>-0.212815</td>\n",
       "      <td>0.004856</td>\n",
       "      <td>0.185074</td>\n",
       "      <td>-0.140132</td>\n",
       "      <td>-0.086794</td>\n",
       "      <td>-0.036400</td>\n",
       "      <td>0.022466</td>\n",
       "      <td>0.083268</td>\n",
       "      <td>0.207589</td>\n",
       "      <td>0.319101</td>\n",
       "      <td>0.595678</td>\n",
       "      <td>1.093037</td>\n",
       "      <td>-19.013763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.207476</td>\n",
       "      <td>-0.007514</td>\n",
       "      <td>0.553831</td>\n",
       "      <td>1.237657</td>\n",
       "      <td>2.177325</td>\n",
       "      <td>1.105210</td>\n",
       "      <td>0.004856</td>\n",
       "      <td>0.185074</td>\n",
       "      <td>-0.140132</td>\n",
       "      <td>-0.086794</td>\n",
       "      <td>-0.036400</td>\n",
       "      <td>0.022466</td>\n",
       "      <td>-0.168848</td>\n",
       "      <td>-0.174526</td>\n",
       "      <td>-0.189613</td>\n",
       "      <td>-0.216587</td>\n",
       "      <td>-0.315684</td>\n",
       "      <td>-10.161019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.532992</td>\n",
       "      <td>-0.581890</td>\n",
       "      <td>-0.591997</td>\n",
       "      <td>-0.523183</td>\n",
       "      <td>-0.418308</td>\n",
       "      <td>-0.212815</td>\n",
       "      <td>0.004856</td>\n",
       "      <td>0.185074</td>\n",
       "      <td>-0.140132</td>\n",
       "      <td>-0.086794</td>\n",
       "      <td>-0.036400</td>\n",
       "      <td>-0.167103</td>\n",
       "      <td>-0.168848</td>\n",
       "      <td>-0.174526</td>\n",
       "      <td>-0.189613</td>\n",
       "      <td>-0.216587</td>\n",
       "      <td>-0.315684</td>\n",
       "      <td>-9.376619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.527745</td>\n",
       "      <td>-0.007514</td>\n",
       "      <td>0.553831</td>\n",
       "      <td>-0.523183</td>\n",
       "      <td>-0.418308</td>\n",
       "      <td>-0.212815</td>\n",
       "      <td>0.004856</td>\n",
       "      <td>0.185074</td>\n",
       "      <td>-0.140132</td>\n",
       "      <td>-0.086794</td>\n",
       "      <td>-0.036400</td>\n",
       "      <td>0.022466</td>\n",
       "      <td>0.083268</td>\n",
       "      <td>0.207589</td>\n",
       "      <td>0.319101</td>\n",
       "      <td>0.595678</td>\n",
       "      <td>1.093037</td>\n",
       "      <td>-13.776438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.526820</td>\n",
       "      <td>-0.007514</td>\n",
       "      <td>-0.591997</td>\n",
       "      <td>-0.523183</td>\n",
       "      <td>-0.418308</td>\n",
       "      <td>-0.212815</td>\n",
       "      <td>0.004856</td>\n",
       "      <td>0.185074</td>\n",
       "      <td>-0.140132</td>\n",
       "      <td>-0.193003</td>\n",
       "      <td>-0.174403</td>\n",
       "      <td>-0.167103</td>\n",
       "      <td>-0.168848</td>\n",
       "      <td>-0.174526</td>\n",
       "      <td>-0.189613</td>\n",
       "      <td>-0.216587</td>\n",
       "      <td>-0.315684</td>\n",
       "      <td>-8.537140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          1        50        99       147       194       240       285  \\\n",
       "0 -0.534448 -0.581890 -0.591997 -0.523183 -0.418308 -0.212815  0.004856   \n",
       "1 -0.207476 -0.007514  0.553831  1.237657  2.177325  1.105210  0.004856   \n",
       "2 -0.532992 -0.581890 -0.591997 -0.523183 -0.418308 -0.212815  0.004856   \n",
       "3 -0.527745 -0.007514  0.553831 -0.523183 -0.418308 -0.212815  0.004856   \n",
       "4 -0.526820 -0.007514 -0.591997 -0.523183 -0.418308 -0.212815  0.004856   \n",
       "\n",
       "        329       840       869       897       924       950       999  \\\n",
       "0  0.185074 -0.140132 -0.086794 -0.036400  0.022466  0.083268  0.207589   \n",
       "1  0.185074 -0.140132 -0.086794 -0.036400  0.022466 -0.168848 -0.174526   \n",
       "2  0.185074 -0.140132 -0.086794 -0.036400 -0.167103 -0.168848 -0.174526   \n",
       "3  0.185074 -0.140132 -0.086794 -0.036400  0.022466  0.083268  0.207589   \n",
       "4  0.185074 -0.140132 -0.193003 -0.174403 -0.167103 -0.168848 -0.174526   \n",
       "\n",
       "       1022      1065      1085        Eat  \n",
       "0  0.319101  0.595678  1.093037 -19.013763  \n",
       "1 -0.189613 -0.216587 -0.315684 -10.161019  \n",
       "2 -0.189613 -0.216587 -0.315684  -9.376619  \n",
       "3  0.319101  0.595678  1.093037 -13.776438  \n",
       "4 -0.189613 -0.216587 -0.315684  -8.537140  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfcopy5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "318ab621-a83c-468e-85b8-dbb03d0a5bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=dfcopy5.drop(\"Eat\",axis=1)\n",
    "y=dfcopy5[\"Eat\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "785c6cbe-6c9e-4f1a-ad38-cfc3861c1f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Lasso()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Lasso</label><div class=\"sk-toggleable__content\"><pre>Lasso()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Lasso()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso_model = Lasso(alpha=1.0) \n",
    "lasso_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8699091a-cb96-496a-9026-71056e6c6f9d",
   "metadata": {},
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_predict=lasso_model.pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "007fff99-e3fa-41b0-a02e-ffa9b2f61cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "y_predict=lasso_model.predict(X_test)\n",
    "Error=mean_absolute_percentage_error(y_test,y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9bb22f96-6372-4a7c-8b1a-d94353313587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25889423026436176"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b578c64d-d7b2-4fd7-bf8a-5a99b436f3ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Ridge()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Ridge()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "Ridge_model = Ridge(alpha=1.0) \n",
    "Ridge_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ae78a742-9f08-4de9-8a1b-add36ebc196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "y_predict_2=Ridge_model.predict(X_test)\n",
    "Error2=mean_absolute_percentage_error(y_test,y_predict_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b50ab8d-7b5b-4164-af0a-5857e6d6de62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17360059988980736"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Error2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e0ac9033-2ccf-4018-84a5-246e58f5e458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "RF_model=RandomForestRegressor()\n",
    "RF_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "36d37ac8-5ff2-4bc2-b0c0-fe5a039facdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07030214145323903"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "y_predict_3=RF_model.predict(X_test)\n",
    "Error3=mean_absolute_percentage_error(y_test,y_predict_3)\n",
    "Error3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "42cc20fc-06e5-445f-9c03-871f744d1868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "273/273 [==============================] - 2s 5ms/step - loss: 143.1957 - mae: 0.2233 - val_loss: 103.9148 - val_mae: 0.1744\n",
      "Epoch 2/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 113.1251 - mae: 0.1622 - val_loss: 136.9693 - val_mae: 0.1550\n",
      "Epoch 3/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 122.9611 - mae: 0.1556 - val_loss: 128.9913 - val_mae: 0.1660\n",
      "Epoch 4/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 117.9679 - mae: 0.1533 - val_loss: 115.3747 - val_mae: 0.1364\n",
      "Epoch 5/200\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 113.4807 - mae: 0.1503 - val_loss: 92.7509 - val_mae: 0.1443\n",
      "Epoch 6/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 113.5566 - mae: 0.1369 - val_loss: 104.5349 - val_mae: 0.1380\n",
      "Epoch 7/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 108.7190 - mae: 0.1370 - val_loss: 83.4157 - val_mae: 0.1270\n",
      "Epoch 8/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 99.7920 - mae: 0.1241 - val_loss: 88.0106 - val_mae: 0.1223\n",
      "Epoch 9/200\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 100.3623 - mae: 0.1245 - val_loss: 94.1762 - val_mae: 0.1292\n",
      "Epoch 10/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 93.7100 - mae: 0.1224 - val_loss: 113.8982 - val_mae: 0.1186\n",
      "Epoch 11/200\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 100.4958 - mae: 0.1230 - val_loss: 97.2251 - val_mae: 0.1327\n",
      "Epoch 12/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 108.2627 - mae: 0.1390 - val_loss: 141.8869 - val_mae: 0.1224\n",
      "Epoch 13/200\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 111.6518 - mae: 0.1165 - val_loss: 70.6127 - val_mae: 0.1122\n",
      "Epoch 14/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 79.6391 - mae: 0.1132 - val_loss: 87.7184 - val_mae: 0.1078\n",
      "Epoch 15/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 101.1165 - mae: 0.1111 - val_loss: 63.0239 - val_mae: 0.1082\n",
      "Epoch 16/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 73.2734 - mae: 0.1076 - val_loss: 107.1217 - val_mae: 0.1123\n",
      "Epoch 17/200\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 96.6168 - mae: 0.1102 - val_loss: 104.2494 - val_mae: 0.1238\n",
      "Epoch 18/200\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 75.2463 - mae: 0.1110 - val_loss: 70.5636 - val_mae: 0.1054\n",
      "Epoch 19/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 92.6188 - mae: 0.1078 - val_loss: 74.0621 - val_mae: 0.1094\n",
      "Epoch 20/200\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 81.1685 - mae: 0.1063 - val_loss: 69.1649 - val_mae: 0.1101\n",
      "Epoch 21/200\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 77.3964 - mae: 0.1079 - val_loss: 72.4744 - val_mae: 0.1006\n",
      "Epoch 22/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 78.0368 - mae: 0.1039 - val_loss: 63.4639 - val_mae: 0.1016\n",
      "Epoch 23/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 69.7959 - mae: 0.1035 - val_loss: 78.9045 - val_mae: 0.1073\n",
      "Epoch 24/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 76.6548 - mae: 0.1024 - val_loss: 73.3895 - val_mae: 0.0988\n",
      "Epoch 25/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 74.1987 - mae: 0.1039 - val_loss: 67.3983 - val_mae: 0.1013\n",
      "Epoch 26/200\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 72.5418 - mae: 0.0999 - val_loss: 81.7008 - val_mae: 0.1010\n",
      "Epoch 27/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 76.5976 - mae: 0.1018 - val_loss: 73.8372 - val_mae: 0.1008\n",
      "Epoch 28/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 70.5021 - mae: 0.0992 - val_loss: 67.8362 - val_mae: 0.1000\n",
      "Epoch 29/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 69.6998 - mae: 0.0990 - val_loss: 77.3056 - val_mae: 0.0999\n",
      "Epoch 30/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 67.7953 - mae: 0.0972 - val_loss: 75.1935 - val_mae: 0.0984\n",
      "Epoch 31/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 71.9302 - mae: 0.0996 - val_loss: 65.2524 - val_mae: 0.1017\n",
      "Epoch 32/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 65.1711 - mae: 0.0975 - val_loss: 68.2739 - val_mae: 0.0992\n",
      "Epoch 33/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 67.9258 - mae: 0.1003 - val_loss: 61.3045 - val_mae: 0.0964\n",
      "Epoch 34/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 63.5044 - mae: 0.0956 - val_loss: 73.7502 - val_mae: 0.0989\n",
      "Epoch 35/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 70.9310 - mae: 0.0982 - val_loss: 76.6453 - val_mae: 0.0986\n",
      "Epoch 36/200\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 68.4687 - mae: 0.0955 - val_loss: 65.2886 - val_mae: 0.0963\n",
      "Epoch 37/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 63.5149 - mae: 0.0933 - val_loss: 85.3141 - val_mae: 0.1110\n",
      "Epoch 38/200\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 79.9117 - mae: 0.0979 - val_loss: 61.1455 - val_mae: 0.1032\n",
      "Epoch 39/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 60.5062 - mae: 0.0968 - val_loss: 96.0754 - val_mae: 0.1051\n",
      "Epoch 40/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 77.6944 - mae: 0.0960 - val_loss: 61.3302 - val_mae: 0.0949\n",
      "Epoch 41/200\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 62.1375 - mae: 0.0974 - val_loss: 67.8652 - val_mae: 0.0967\n",
      "Epoch 42/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 69.1239 - mae: 0.0956 - val_loss: 58.8642 - val_mae: 0.1006\n",
      "Epoch 43/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 58.0104 - mae: 0.0945 - val_loss: 67.6087 - val_mae: 0.0988\n",
      "Epoch 44/200\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 67.5406 - mae: 0.0956 - val_loss: 63.4691 - val_mae: 0.0977\n",
      "Epoch 45/200\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 61.4688 - mae: 0.0970 - val_loss: 59.2349 - val_mae: 0.0969\n",
      "Epoch 46/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 61.4655 - mae: 0.0937 - val_loss: 63.9882 - val_mae: 0.0934\n",
      "Epoch 47/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 66.6421 - mae: 0.0926 - val_loss: 68.6454 - val_mae: 0.1018\n",
      "Epoch 48/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 62.9633 - mae: 0.0965 - val_loss: 57.9537 - val_mae: 0.0978\n",
      "Epoch 49/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 60.7614 - mae: 0.0930 - val_loss: 63.4924 - val_mae: 0.0999\n",
      "Epoch 50/200\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 60.9900 - mae: 0.0924 - val_loss: 59.3577 - val_mae: 0.0955\n",
      "Epoch 51/200\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 57.2219 - mae: 0.0937 - val_loss: 64.5385 - val_mae: 0.0971\n",
      "Epoch 52/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 64.2152 - mae: 0.0933 - val_loss: 58.0156 - val_mae: 0.0951\n",
      "Epoch 53/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 58.0104 - mae: 0.0922 - val_loss: 64.2663 - val_mae: 0.0922\n",
      "Epoch 54/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 59.9824 - mae: 0.0911 - val_loss: 57.6252 - val_mae: 0.0952\n",
      "Epoch 55/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 56.7054 - mae: 0.0929 - val_loss: 58.9924 - val_mae: 0.0982\n",
      "Epoch 56/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 59.8469 - mae: 0.0930 - val_loss: 53.4450 - val_mae: 0.0923\n",
      "Epoch 57/200\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 53.9763 - mae: 0.0903 - val_loss: 60.7140 - val_mae: 0.0974\n",
      "Epoch 58/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 60.1422 - mae: 0.0909 - val_loss: 53.5343 - val_mae: 0.0943\n",
      "Epoch 59/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 56.5965 - mae: 0.0909 - val_loss: 54.3727 - val_mae: 0.0926\n",
      "Epoch 60/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 54.2066 - mae: 0.0912 - val_loss: 55.4331 - val_mae: 0.0894\n",
      "Epoch 61/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 56.4022 - mae: 0.0906 - val_loss: 52.9895 - val_mae: 0.0908\n",
      "Epoch 62/200\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 53.5889 - mae: 0.0903 - val_loss: 55.9158 - val_mae: 0.0926\n",
      "Epoch 63/200\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 54.8282 - mae: 0.0891 - val_loss: 54.4279 - val_mae: 0.0902\n",
      "Epoch 64/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 61.0023 - mae: 0.0908 - val_loss: 55.0538 - val_mae: 0.0918\n",
      "Epoch 65/200\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 55.4621 - mae: 0.0902 - val_loss: 53.8446 - val_mae: 0.0948\n",
      "Epoch 66/200\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 52.8470 - mae: 0.0904 - val_loss: 54.9149 - val_mae: 0.0932\n",
      "Epoch 67/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 53.6132 - mae: 0.0892 - val_loss: 54.3237 - val_mae: 0.0915\n",
      "Epoch 68/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 52.9149 - mae: 0.0903 - val_loss: 54.0072 - val_mae: 0.0942\n",
      "Epoch 69/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 53.7794 - mae: 0.0896 - val_loss: 54.1049 - val_mae: 0.0899\n",
      "Epoch 70/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 53.0868 - mae: 0.0898 - val_loss: 53.5602 - val_mae: 0.0934\n",
      "Epoch 71/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 52.9060 - mae: 0.0891 - val_loss: 54.5629 - val_mae: 0.0927\n",
      "Epoch 72/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 53.5350 - mae: 0.0899 - val_loss: 53.2161 - val_mae: 0.0919\n",
      "Epoch 73/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 51.7989 - mae: 0.0889 - val_loss: 54.5736 - val_mae: 0.0916\n",
      "Epoch 74/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 53.9527 - mae: 0.0886 - val_loss: 52.2344 - val_mae: 0.0925\n",
      "Epoch 75/200\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 51.4480 - mae: 0.0880 - val_loss: 53.3641 - val_mae: 0.0910\n",
      "Epoch 76/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 53.4688 - mae: 0.0887 - val_loss: 51.9280 - val_mae: 0.0932\n",
      "Epoch 77/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 51.5396 - mae: 0.0885 - val_loss: 54.4369 - val_mae: 0.0886\n",
      "Epoch 78/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 52.4256 - mae: 0.0876 - val_loss: 50.4110 - val_mae: 0.0899\n",
      "Epoch 79/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 50.5324 - mae: 0.0882 - val_loss: 51.2719 - val_mae: 0.0897\n",
      "Epoch 80/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 51.8975 - mae: 0.0870 - val_loss: 51.6092 - val_mae: 0.0920\n",
      "Epoch 81/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 53.7615 - mae: 0.0889 - val_loss: 51.3388 - val_mae: 0.0890\n",
      "Epoch 82/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 50.9817 - mae: 0.0866 - val_loss: 49.8753 - val_mae: 0.0895\n",
      "Epoch 83/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 50.3009 - mae: 0.0877 - val_loss: 52.2304 - val_mae: 0.0924\n",
      "Epoch 84/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 50.4755 - mae: 0.0869 - val_loss: 52.3330 - val_mae: 0.0891\n",
      "Epoch 85/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 50.3182 - mae: 0.0867 - val_loss: 50.4871 - val_mae: 0.0907\n",
      "Epoch 86/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 50.2361 - mae: 0.0869 - val_loss: 52.0072 - val_mae: 0.0931\n",
      "Epoch 87/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 49.7228 - mae: 0.0863 - val_loss: 50.3057 - val_mae: 0.0898\n",
      "Epoch 88/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 50.3579 - mae: 0.0864 - val_loss: 52.0208 - val_mae: 0.0910\n",
      "Epoch 89/200\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 51.4277 - mae: 0.0868 - val_loss: 52.0933 - val_mae: 0.0920\n",
      "Epoch 90/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 50.5107 - mae: 0.0862 - val_loss: 51.2325 - val_mae: 0.0914\n",
      "Epoch 91/200\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 49.9536 - mae: 0.0865 - val_loss: 50.0089 - val_mae: 0.0902\n",
      "Epoch 92/200\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 49.4603 - mae: 0.0852 - val_loss: 51.9430 - val_mae: 0.0896\n",
      "Epoch 93/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 49.5415 - mae: 0.0853 - val_loss: 49.9437 - val_mae: 0.0900\n",
      "Epoch 94/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 49.0233 - mae: 0.0859 - val_loss: 50.5179 - val_mae: 0.0899\n",
      "Epoch 95/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 48.8880 - mae: 0.0856 - val_loss: 50.4263 - val_mae: 0.0855\n",
      "Epoch 96/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 48.5562 - mae: 0.0845 - val_loss: 51.3327 - val_mae: 0.0851\n",
      "Epoch 97/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 49.5299 - mae: 0.0851 - val_loss: 49.9081 - val_mae: 0.0888\n",
      "Epoch 98/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 49.9541 - mae: 0.0863 - val_loss: 50.3766 - val_mae: 0.0858\n",
      "Epoch 99/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 48.7322 - mae: 0.0852 - val_loss: 50.7618 - val_mae: 0.0922\n",
      "Epoch 100/200\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 48.6336 - mae: 0.0858 - val_loss: 49.7275 - val_mae: 0.0870\n",
      "Epoch 101/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 48.5117 - mae: 0.0849 - val_loss: 50.0448 - val_mae: 0.0858\n",
      "Epoch 102/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 49.4641 - mae: 0.0848 - val_loss: 49.5564 - val_mae: 0.0897\n",
      "Epoch 103/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 48.5343 - mae: 0.0844 - val_loss: 50.6569 - val_mae: 0.0849\n",
      "Epoch 104/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 48.1259 - mae: 0.0844 - val_loss: 50.8878 - val_mae: 0.0849\n",
      "Epoch 105/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 48.8268 - mae: 0.0837 - val_loss: 49.3391 - val_mae: 0.0889\n",
      "Epoch 106/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 47.9219 - mae: 0.0835 - val_loss: 48.8059 - val_mae: 0.0851\n",
      "Epoch 107/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 48.3031 - mae: 0.0837 - val_loss: 49.9626 - val_mae: 0.0868\n",
      "Epoch 108/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 47.7673 - mae: 0.0832 - val_loss: 49.9727 - val_mae: 0.0878\n",
      "Epoch 109/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 48.7598 - mae: 0.0836 - val_loss: 50.3602 - val_mae: 0.0871\n",
      "Epoch 110/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 47.4876 - mae: 0.0831 - val_loss: 50.8858 - val_mae: 0.0858\n",
      "Epoch 111/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 47.9500 - mae: 0.0826 - val_loss: 50.5379 - val_mae: 0.0898\n",
      "Epoch 112/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 47.6898 - mae: 0.0837 - val_loss: 49.5993 - val_mae: 0.0862\n",
      "Epoch 113/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 47.3263 - mae: 0.0838 - val_loss: 49.6517 - val_mae: 0.0855\n",
      "Epoch 114/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 48.3560 - mae: 0.0836 - val_loss: 50.1250 - val_mae: 0.0854\n",
      "Epoch 115/200\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 47.5247 - mae: 0.0834 - val_loss: 51.3860 - val_mae: 0.0876\n",
      "Epoch 116/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 49.5890 - mae: 0.0851 - val_loss: 51.0842 - val_mae: 0.0847\n",
      "Epoch 117/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 47.9560 - mae: 0.0837 - val_loss: 49.9005 - val_mae: 0.0888\n",
      "Epoch 118/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 47.6552 - mae: 0.0847 - val_loss: 48.9334 - val_mae: 0.0851\n",
      "Epoch 119/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 47.5866 - mae: 0.0834 - val_loss: 48.9253 - val_mae: 0.0851\n",
      "Epoch 120/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 47.5546 - mae: 0.0832 - val_loss: 49.1570 - val_mae: 0.0854\n",
      "Epoch 121/200\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 47.1207 - mae: 0.0831 - val_loss: 48.9911 - val_mae: 0.0879\n",
      "Epoch 122/200\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 47.2359 - mae: 0.0835 - val_loss: 49.0536 - val_mae: 0.0863\n",
      "Epoch 123/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 46.6473 - mae: 0.0822 - val_loss: 48.4088 - val_mae: 0.0858\n",
      "Epoch 124/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 46.8900 - mae: 0.0827 - val_loss: 49.4344 - val_mae: 0.0867\n",
      "Epoch 125/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 46.9634 - mae: 0.0825 - val_loss: 48.9632 - val_mae: 0.0874\n",
      "Epoch 126/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 47.1848 - mae: 0.0827 - val_loss: 49.2424 - val_mae: 0.0856\n",
      "Epoch 127/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 47.9253 - mae: 0.0824 - val_loss: 51.1417 - val_mae: 0.0928\n",
      "Epoch 128/200\n",
      "273/273 [==============================] - 1s 5ms/step - loss: 47.7266 - mae: 0.0840 - val_loss: 50.1643 - val_mae: 0.0866\n",
      "Epoch 129/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 46.5087 - mae: 0.0825 - val_loss: 50.2563 - val_mae: 0.0856\n",
      "Epoch 130/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 47.0463 - mae: 0.0825 - val_loss: 49.5519 - val_mae: 0.0871\n",
      "Epoch 131/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 46.9358 - mae: 0.0824 - val_loss: 49.1153 - val_mae: 0.0860\n",
      "Epoch 132/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 46.2748 - mae: 0.0822 - val_loss: 49.1783 - val_mae: 0.0857\n",
      "Epoch 133/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 47.1242 - mae: 0.0832 - val_loss: 51.1722 - val_mae: 0.0871\n",
      "Epoch 134/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 46.4361 - mae: 0.0824 - val_loss: 50.9845 - val_mae: 0.0873\n",
      "Epoch 135/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 47.2367 - mae: 0.0821 - val_loss: 52.3285 - val_mae: 0.0915\n",
      "Epoch 136/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 46.8898 - mae: 0.0837 - val_loss: 48.7316 - val_mae: 0.0878\n",
      "Epoch 137/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 46.4348 - mae: 0.0831 - val_loss: 49.7492 - val_mae: 0.0857\n",
      "Epoch 138/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 45.9537 - mae: 0.0819 - val_loss: 49.3057 - val_mae: 0.0854\n",
      "Epoch 139/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 46.2560 - mae: 0.0812 - val_loss: 48.5036 - val_mae: 0.0865\n",
      "Epoch 140/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 45.5229 - mae: 0.0821 - val_loss: 49.6267 - val_mae: 0.0836\n",
      "Epoch 141/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 46.4261 - mae: 0.0823 - val_loss: 50.4644 - val_mae: 0.0858\n",
      "Epoch 142/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 46.4996 - mae: 0.0813 - val_loss: 49.8517 - val_mae: 0.0856\n",
      "Epoch 143/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 46.3785 - mae: 0.0822 - val_loss: 50.5497 - val_mae: 0.0877\n",
      "Epoch 144/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 45.4662 - mae: 0.0812 - val_loss: 49.1748 - val_mae: 0.0840\n",
      "Epoch 145/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 45.1914 - mae: 0.0809 - val_loss: 49.2512 - val_mae: 0.0858\n",
      "Epoch 146/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 45.5200 - mae: 0.0813 - val_loss: 49.8944 - val_mae: 0.0845\n",
      "Epoch 147/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 45.3035 - mae: 0.0809 - val_loss: 49.8996 - val_mae: 0.0851\n",
      "Epoch 148/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 45.9006 - mae: 0.0820 - val_loss: 51.5846 - val_mae: 0.0864\n",
      "Epoch 149/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 48.2713 - mae: 0.0835 - val_loss: 50.6495 - val_mae: 0.0864\n",
      "Epoch 150/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 46.2271 - mae: 0.0817 - val_loss: 50.6001 - val_mae: 0.0850\n",
      "Epoch 151/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 46.0125 - mae: 0.0817 - val_loss: 49.0014 - val_mae: 0.0851\n",
      "Epoch 152/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 45.4396 - mae: 0.0811 - val_loss: 49.4769 - val_mae: 0.0856\n",
      "Epoch 153/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 45.2790 - mae: 0.0807 - val_loss: 50.7762 - val_mae: 0.0869\n",
      "Epoch 154/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 46.9100 - mae: 0.0816 - val_loss: 50.0540 - val_mae: 0.0863\n",
      "Epoch 155/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 46.5938 - mae: 0.0824 - val_loss: 49.1467 - val_mae: 0.0855\n",
      "Epoch 156/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 46.1512 - mae: 0.0817 - val_loss: 48.8017 - val_mae: 0.0869\n",
      "Epoch 157/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 45.4710 - mae: 0.0821 - val_loss: 48.6649 - val_mae: 0.0849\n",
      "Epoch 158/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 45.1538 - mae: 0.0812 - val_loss: 48.1348 - val_mae: 0.0852\n",
      "Epoch 159/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 45.5782 - mae: 0.0819 - val_loss: 48.2909 - val_mae: 0.0858\n",
      "Epoch 160/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 45.0810 - mae: 0.0811 - val_loss: 48.6566 - val_mae: 0.0840\n",
      "Epoch 161/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 45.1494 - mae: 0.0812 - val_loss: 48.1839 - val_mae: 0.0843\n",
      "Epoch 162/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 45.1195 - mae: 0.0809 - val_loss: 48.2544 - val_mae: 0.0857\n",
      "Epoch 163/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 45.3958 - mae: 0.0805 - val_loss: 49.0424 - val_mae: 0.0863\n",
      "Epoch 164/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 45.1043 - mae: 0.0804 - val_loss: 50.5602 - val_mae: 0.0850\n",
      "Epoch 165/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 45.4448 - mae: 0.0815 - val_loss: 49.4713 - val_mae: 0.0848\n",
      "Epoch 166/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 44.8239 - mae: 0.0799 - val_loss: 48.5060 - val_mae: 0.0841\n",
      "Epoch 167/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 44.7060 - mae: 0.0813 - val_loss: 49.8011 - val_mae: 0.0860\n",
      "Epoch 168/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 45.4103 - mae: 0.0810 - val_loss: 49.0790 - val_mae: 0.0849\n",
      "Epoch 169/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 44.7344 - mae: 0.0801 - val_loss: 51.1888 - val_mae: 0.0838\n",
      "Epoch 170/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 45.6044 - mae: 0.0803 - val_loss: 48.6503 - val_mae: 0.0847\n",
      "Epoch 171/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 44.7410 - mae: 0.0803 - val_loss: 48.4762 - val_mae: 0.0828\n",
      "Epoch 172/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 44.4832 - mae: 0.0797 - val_loss: 49.7167 - val_mae: 0.0834\n",
      "Epoch 173/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 44.5994 - mae: 0.0803 - val_loss: 49.5036 - val_mae: 0.0834\n",
      "Epoch 174/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 44.8974 - mae: 0.0803 - val_loss: 48.0372 - val_mae: 0.0862\n",
      "Epoch 175/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 45.0506 - mae: 0.0803 - val_loss: 48.4375 - val_mae: 0.0849\n",
      "Epoch 176/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 45.7752 - mae: 0.0817 - val_loss: 50.2630 - val_mae: 0.0874\n",
      "Epoch 177/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 44.8534 - mae: 0.0808 - val_loss: 48.8502 - val_mae: 0.0833\n",
      "Epoch 178/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 45.0787 - mae: 0.0798 - val_loss: 49.0558 - val_mae: 0.0849\n",
      "Epoch 179/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 44.1913 - mae: 0.0797 - val_loss: 49.2073 - val_mae: 0.0830\n",
      "Epoch 180/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 45.1046 - mae: 0.0802 - val_loss: 49.1382 - val_mae: 0.0841\n",
      "Epoch 181/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 44.9267 - mae: 0.0806 - val_loss: 48.7054 - val_mae: 0.0841\n",
      "Epoch 182/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 44.3210 - mae: 0.0790 - val_loss: 49.2619 - val_mae: 0.0842\n",
      "Epoch 183/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 45.3903 - mae: 0.0818 - val_loss: 48.8491 - val_mae: 0.0841\n",
      "Epoch 184/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 45.3510 - mae: 0.0803 - val_loss: 50.5037 - val_mae: 0.0864\n",
      "Epoch 185/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 45.8647 - mae: 0.0811 - val_loss: 48.6683 - val_mae: 0.0837\n",
      "Epoch 186/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 44.4267 - mae: 0.0801 - val_loss: 49.7657 - val_mae: 0.0833\n",
      "Epoch 187/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 44.5258 - mae: 0.0795 - val_loss: 48.8888 - val_mae: 0.0853\n",
      "Epoch 188/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 44.3467 - mae: 0.0798 - val_loss: 49.5564 - val_mae: 0.0861\n",
      "Epoch 189/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 44.5584 - mae: 0.0797 - val_loss: 49.1357 - val_mae: 0.0855\n",
      "Epoch 190/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 44.5553 - mae: 0.0798 - val_loss: 48.7771 - val_mae: 0.0849\n",
      "Epoch 191/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 44.2044 - mae: 0.0792 - val_loss: 48.3926 - val_mae: 0.0846\n",
      "Epoch 192/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 43.8511 - mae: 0.0791 - val_loss: 49.8236 - val_mae: 0.0853\n",
      "Epoch 193/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 44.1570 - mae: 0.0794 - val_loss: 48.8708 - val_mae: 0.0831\n",
      "Epoch 194/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 44.5751 - mae: 0.0798 - val_loss: 48.8638 - val_mae: 0.0832\n",
      "Epoch 195/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 44.0056 - mae: 0.0791 - val_loss: 48.4521 - val_mae: 0.0825\n",
      "Epoch 196/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 44.7803 - mae: 0.0792 - val_loss: 49.8351 - val_mae: 0.0834\n",
      "Epoch 197/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 44.5257 - mae: 0.0803 - val_loss: 49.7699 - val_mae: 0.0851\n",
      "Epoch 198/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 43.7264 - mae: 0.0787 - val_loss: 49.9361 - val_mae: 0.0861\n",
      "Epoch 199/200\n",
      "273/273 [==============================] - 1s 3ms/step - loss: 43.7799 - mae: 0.0791 - val_loss: 48.2723 - val_mae: 0.0839\n",
      "Epoch 200/200\n",
      "273/273 [==============================] - 1s 4ms/step - loss: 43.7996 - mae: 0.0787 - val_loss: 49.6584 - val_mae: 0.0854\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanAbsolutePercentageError\n",
    "\n",
    "# Initialize the Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer and first hidden layer\n",
    "model.add(Dense(64, activation='relu', input_shape=(17,)))  # Adjust '17' to the number of features\n",
    "\n",
    "# Second hidden layer\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# Output layer for regression\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model with MAPE as the loss function\n",
    "model.compile(optimizer=Adam(learning_rate=0.0005),\n",
    "              loss=MeanAbsolutePercentageError(),  # Use Keras MAPE\n",
    "              metrics=['mae'])  # Include MAE as an additional metric\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train_scaled, epochs=200, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "09b990fb-86e3-4da1-99bc-5d17a5b921db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168/168 [==============================] - 1s 3ms/step\n",
      "Scaled MAPE on test data: 0.09601369679556028\n"
     ]
    }
   ],
   "source": [
    "y_pred_scaled = model.predict(X_test)\n",
    "y_pred_dp = scaler.inverse_transform(y_pred_scaled)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Scaled MAPE on test data:\", mean_absolute_percentage_error(y_test, y_pred_dp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28230373-24b3-40d8-9632-4e6648340222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
